[ { "title": "[TIL] 99클럽 코테 스터디 6기 9일차 TIL - [그리디] 저울", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%949%EC%9D%BC%EC%B0%A8/", "categories": "TIL, 코테", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-10 00:00:00 +0900", "snippet": "오늘의 문제2437. 저울(골드2)키워드그리디, 정렬나의 풀이 N = int(input())weights = list(map(int, input().split()))weights.sort()sum_weight = 0for i in range(N) : if sum_weight + 1 &amp;gt;= weights[i] : sum_weight += weights[i] else : breakprint(sum_weight+1) 회고" }, { "title": "[TIL] 99클럽 코테 스터디 6기 8일차 TIL - [정규표현식] 한국이 그리울 땐 서버에 접속하지", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%948%EC%9D%BC%EC%B0%A8/", "categories": "TIL, 코테", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-09 00:00:00 +0900", "snippet": "오늘의 문제9996. 한국이 그리울 땐 서버에 접속하지(실버3)키워드문자열, 정규표현식나의 풀이 ## 더 간단하게 수정N = int(input())prefix, suffix = input().split(&#39;*&#39;)for _ in range(N): filename = input() print(&quot;DA&quot; if len(filename) &amp;gt;= len(prefix) + len(suffix) and filename.startswith(prefix) and filename.endswith(suffix) else &quot;NE&quot;)## [수정 후 정답]N = int(input())pattern_prefix, pattern_suffix = input().split(&#39;*&#39;)files = [input() for _ in range(N)]for filename in files : if len(filename) &amp;lt; len(pattern_prefix) + len(pattern_suffix) : print(&quot;NE&quot;) elif filename[:len(pattern_prefix)] == pattern_prefix and filename[-len(pattern_suffix):] == pattern_suffix : print(&quot;DA&quot;) else : print(&quot;NE&quot;)## [수정 전 오답]pattern = input()for file in files : if file[0]+file[-1] == pattern[0]+pattern[-1] : print(&quot;DA&quot;) else : print(&quot;NE&quot;) 회고" }, { "title": "[TIL] 99클럽 코테 스터디 6기 7일차 TIL - [스택] 쇠 막대기", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%947%EC%9D%BC%EC%B0%A8/", "categories": "TIL, 코테", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-08 00:00:00 +0900", "snippet": "오늘의 문제10799. 쇠 막대기(실버2)키워드자료구조, 스택나의 풀이 brackets = input()stack = []count = 0for i in range(len(brackets)) : if brackets[i] == &#39;(&#39; : stack.append(brackets[i]) else : stack.pop() if brackets[i-1] == &#39;(&#39; : ## 직전 값이 여는 괄호(=레이저) count += len(stack) ## 막대기 조각 개수 else : ## 직전 값이 닫는 괄호(=막대기 끝) count += 1print(count)# [수정 전 오답]brackets = list(input())stack = []count = 0for s in brackets : if len(stack) == 0 or s == &#39;(&#39; : stack.append(s) elif stack[-1] == &#39;(&#39; and s == &#39;)&#39; : ## 레이저: 직전 값이 여는 괄호, 현재 값이 닫는 괄호일 때 stack.pop() ## 여는 괄호 1개 제거 count += stack.count(&#39;(&#39;) elif s == &#39;)&#39; : # 막대기 오른쪽 끝 stack.pop() count += 1print(count) 회고" }, { "title": "[TIL] 99클럽 코테 스터디 6기 6일차 TIL - [BFS/DFS] 섬의 개수", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%946%EC%9D%BC%EC%B0%A8/", "categories": "TIL, 코테", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-07 00:00:00 +0900", "snippet": "오늘의 문제4963. 섬의 개수(실버2)키워드그래프, BFS/DFS나의 풀이 회고" }, { "title": "[TIL] 99클럽 코테 스터디 6기 5일차 TIL - 수열(슬라이딩 윈도우)", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%945%EC%9D%BC%EC%B0%A8/", "categories": "TIL, 코테", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-04 00:00:00 +0900", "snippet": "오늘의 문제2559. 수열(실버1)키워드누적 합, 두 포인터, 슬라이딩 윈도우나의 풀이 ## 수정 후 정답(슬라이딩 윈도우 사용)N, K = map(int, input().split())arr = list(map(int, input().split()))# 초기 윈도우 합window_sum = sum(arr[:K])max_sum = window_sum# 한 칸씩 이동하면서 합 갱신for i in range(K, N): window_sum += arr[i] - arr[i - K] max_sum = max(max_sum, window_sum) ## 둘 중 더 큰 값을 저장print(max_sum)## [수정 전 오답] 시간초과N, K = map(int, input().split())arr = [*map(int, input().split())]max = 0for i in range(N-K+1) : sum_arr = sum(arr[i:i+K]) if max &amp;lt; sum_arr : max = sum_arr print(max)## [수정 전 오답] 시간초과print(max([sum(arr[i:i+K]) for i in range(N-K+1)])) 회고" }, { "title": "[TIL] 99클럽 코테 스터디 6기 4일차 TIL - [그래프] 안전 영역", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%944%EC%9D%BC%EC%B0%A8/", "categories": "TIL, 코테", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-03 00:00:00 +0900", "snippet": "오늘의 문제2468. 안전 영역(실버1)키워드그래프, 완전탐색, BFS/DFS, Brute force나의 풀이 N, M = map(int, input().split())r, c, d = map(int, input().split())grid = [input().split() for _ in range(N)]print(grid) 회고" }, { "title": "[TIL] 99클럽 코테 스터디 6기 3일차 TIL - [배열] 바탕화면 정리", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%943%EC%9D%BC%EC%B0%A8/", "categories": "TIL, 코테", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-02 00:00:00 +0900", "snippet": "오늘의 문제바탕화면 정리(lv1)키워드나의 풀이 def solution(wallpaper): rdx = rdy = 0 first = True ## 초기값 설정 for i in range(len(wallpaper)) : for j in range(len(wallpaper[0])) : if wallpaper[i][j] == &quot;#&quot; : ## 시작점 설정 if first == True : lux = i luy = j first = False ## 시작점 업데이트 if lux &amp;lt; i : lux = min(lux, i) luy = min(luy, j) ## 끝점 업데이트 if rdx &amp;lt; max(rdx, i+1) or rdy &amp;lt; max(rdy, j+1) : rdx = max(rdx, i+1) rdy = max(rdy, j+1) answer = [lux, luy, rdx, rdy] return answer &amp;lt;\\div&amp;gt; 회고 lv1인데도 시간이 꽤 오래 걸렸다,, 이번주 내내 너무 피곤한 상태여서(머리를 너무 많이 쓴 듯) 생각하는대로 구현도 잘 안되고..주말부터 다시 제대로 공부하면서 풀어봐야겠다. (+이번 문제 더 간단한 코드로 정리하기)" }, { "title": "[TIL] 99클럽 코테 스터디 6기 2일차 TIL - [DP] 피보나치 수열", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%942%EC%9D%BC%EC%B0%A8/", "categories": "TIL", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-04-01 00:00:00 +0900", "snippet": "오늘의 문제14495. 피보나치 비스무리한 수열(실버4)키워드다이나믹 프로그래밍, 피보나치 수열나의 풀이 n = int(input())d = [0]*117 ## n의 범위(1 이상, 116 이하)d[1] = d[2] = d[3] = 1 ## f(1)=f(2)=f(3) = 1for i in range(4, n+1): d[i] = d[i-1]+d[i-3]print(d[n]) &amp;lt;\\div&amp;gt; 회고" }, { "title": "[TIL] 99클럽 코테 스터디 6기 1일차 TIL - 에라토스테네스의 체", "url": "/posts/TIL-%ED%95%AD%ED%95%B4%EC%BD%94%ED%85%8C%EC%8A%A4%ED%84%B0%EB%94%941%EC%9D%BC%EC%B0%A8/", "categories": "TIL", "tags": "TIL, 코딩테스트준비, 항해99, 99클럽, 개발자취업", "date": "2025-03-31 00:00:00 +0900", "snippet": "오늘의 문제1929. 소수 구기(실버3)키워드소수 구하기, 에라토스테네스의 체나의 풀이 M, N = map(int, input().split())## 수정 후 정답for i in range(M, N+1): ## 소수 검사 if i &amp;gt;= 2 : for j in range(2, int(i ** 0.5) + 1): if i % j == 0: break else: print(i)## [수정 전 오답] 시간초과num = Nwhile num &amp;lt;= M : isPrime = True for i in range(2, num) : if num % i == 0 : isPrime = False break if isPrime : print(num) num += 1 &amp;lt;\\div&amp;gt; 회고 몇개월 전에 프로그래머스에서 ‘소수 찾기’ 문제를 풀면서 처음 ‘에라토스테네스의 체’ 개념을 접했는데, 한동안 코테 준비를 안했더니 개념은 잊어버리고 그걸 사용해야 한다는 것..만 아는 사람이 되었다." }, { "title": "[LG Aimers] 딥러닝 기초 3. 과적합 실습(Python)", "url": "/posts/TIL-%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EC%B4%883/", "categories": "TIL, lecture", "tags": "TIL, AI, LGAimers", "date": "2024-11-26 00:00:00 +0900", "snippet": "LG Aimers 5기 AI 실습 교육 수강 내용을 정리합니다. 모든 자료는 교육 플랫폼 앨리스로부터 제공받았습니다.[ 과적합(Overfitting) ] 과적합(Overfitting)이란?모델이 학습 데이터에만 너무 치중되어 학습 데이터에 대한 예측 성능은 좋지만 테스트 데이터에 대해서는 성능이 떨어지는 경우(=일반화되지 않은 모델이라고도 함) 과적합이 발생하는 원인 데이터의 퍼진 정도, 즉 분산(variance)이 높은 경우 너무 많이 학습 데이터를 학습시킨 경우 (epochs가 매우 큰 경우) 학습에 사용된 파라미터가 너무 많은 경우 데이터에 비해 모델이 너무 복잡한 경우 데이터에 노이즈 &amp;amp; 이상치(outlier)가 너무 많은 경우 [ 과적합 - IMDB 영화 리뷰 데이터셋으로 일반 모델과 과적합 모델의 loss 그래프 확인 ] import numpy as npimport tensorflow as tffrom visual import *## 데이터를 전처리하는 함수def sequences_shaping(sequences, dimension): results = np.zeros((len(sequences), dimension)) for i, word_indices in enumerate(sequences): results[i, word_indices] = 1.0 return results ## 1. 과적합 될 모델과 비교하기 위해 기본 모델을 마크다운 설명과 동일하게 생성def Basic(word_num): basic_model = basic_model = tf.keras.Sequential([ tf.keras.layers.Dense(256, activation = &#39;relu&#39;, input_shape=(word_num,)), tf.keras.layers.Dense(128, activation = &#39;relu&#39;),tf.keras.layers.Dense(1, activation= &#39;sigmoid&#39;)]) return basic_model## 2. 기본 모델의 레이어 수와 노드 수를 자유롭게 늘려서 과적합 될 모델을 생성def Overfitting(word_num): overfit_model = basic_model = tf.keras.Sequential([ tf.keras.layers.Dense(1024, activation = &#39;relu&#39;, input_shape=(word_num,)), tf.keras.layers.Dense(512, activation = &#39;relu&#39;), tf.keras.layers.Dense(512, activation = &#39;relu&#39;), tf.keras.layers.Dense(512, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation= &#39;sigmoid&#39;)]) return overfit_model## 3. 두 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가def main(): word_num = 100 data_num = 25000 # imdb 데이터셋 전처리 (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num) train_data = sequences_shaping(train_data, dimension = word_num) test_data = sequences_shaping(test_data, dimension = word_num) basic_model = Basic(word_num) # 기본 모델입니다. overfit_model = Overfitting(word_num) # 과적합시킬 모델입니다. basic_model.compile(loss = &#39;binary_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;accuracy&#39;, &#39;binary_crossentropy&#39;]) overfit_model.compile(loss = &#39;binary_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;accuracy&#39;, &#39;binary_crossentropy&#39;]) basic_model.summary() overfit_model.summary() basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batchsio = 500, validation_data = (test_data, test_labels), verbose = 0) print(&#39;\\n&#39;) overfit_history = overfit_model.fit(train_data, train_labels, epochs = 300, batchsio = 500, validation_data = (test_data, test_labels), verbose = 0) scores_basic = basic_model.evaluate(test_data, test_labels, verbose = 0) scores_overfit = overfit_model.evaluate(test_data, test_labels, verbose = 0) print(&#39;\\nscores_basic: &#39;, scores_basic[-1]) print(&#39;scores_overfit: &#39;, scores_overfit[-1]) Visualize([(&#39;Basic&#39;, basic_history),(&#39;Overfitting&#39;, overfit_history)]) return basic_history, overfit_history [ 과적합(Overfitting) 방지 기법 ] 정규화(Regularization) 모델이 복잡해질수록 파라미터 수가 많아지면서 weight 합의 절댓값이 커지는 경향이 발생함 -&amp;gt; 이를 이용해서 overfitting을 예측할 수 있음(절댓값이 크면 과적합 된 것) 기존 loss 함수에 규제항을 더해서 최적값을 찾을 수 있음 L1 정규화(Lasso Regularization) 규제항(penalty term): 가중치의 절댓값의 합 규제항을 적용하면 작은 가중치들이 더 작아져서 몇개의 중요한 가중치만 남음 L2 정규화(Ridge Regularization) 규제항(penalty term): 가중치의 제곱의 합 L1 정규화에 비해 0으로 수렴하는 가중치가 적음(큰 값을 가진 가중치를 더욱 제약하는 효과 있음) 드롭아웃(Dropout) 각 레이어마다 일정 비율의 뉴런을 임의로 drop시키고 나머지 뉴런만 학습 -&amp;gt; 모델 복잡도가 낮아짐 드롭아웃 적용 시, 학습되는 노드와 가중치들이 매번 달라짐 다른 정규화 기법들과 상호보완적으로 사용 가능함 배치 정규화(Batch Normalization) normalization을 처음 입력 데이터 뿐만 아니라 신경망 내부의 hidden layer의 input에도 적용함 각 레이어마다 정규화를 진행하므로 가중치의 초기값에 크게 의존하지 않음 드롭아웃이나 L1, L2 정규화 대신 많이 사용함 [ 과적합 - L1, L2 Regularization] ## 1. L1, L2 정규화를 적용한 모델과 비교하기 위한 기본 모델 생성 def Basic(word_num): basic_model = tf.keras.Sequential([tf.keras.layers.Dense(128, input_shape = (word_num,), activation = &#39;relu&#39;), tf.keras.layers.Dense(128, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)]) return basic_model ## 2. 기본 모델에 L1 정규화를 입력층과 히든층에만 적용 def L1(word_num): l1_model = tf.keras.Sequential([tf.keras.layers.Dense(128, input_shape = (word_num,), activation = &#39;relu&#39;, kernel_regularizer = tf.keras.regularizers.l1(0.001)), tf.keras.layers.Dense(128, activation = &#39;relu&#39;, kernel_regularizer = tf.keras.regularizers.l1(0.001)), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)]) return l1_model ## 3. 기본 모델에 L2 정규화를 입력층과 히든층에만 적용 def L2(word_num): l2_model = tf.keras.Sequential([tf.keras.layers.Dense(128, input_shape = (word_num,), activation = &#39;relu&#39;, kernel_regularizer = tf.keras.regularizers.l2(0.001)), tf.keras.layers.Dense(128, activation = &#39;relu&#39;, kernel_regularizer = tf.keras.regularizers.l2(0.001)), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)]) return l2_model [ 과적합 - Dropout ] ## 1. 드롭 아웃을 적용할 모델과 비교하기 위한 기본 모델 생성 def Basic(word_num): basic_model = tf.keras.Sequential([ tf.keras.layers.Dense(128, input_shape = (word_num,), activation = &#39;relu&#39;), tf.keras.layers.Dense(128, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)]) return basic_model ## 2. 기본 모델에 드롭 아웃 레이어를 추가(일반적으로 마지막 히든층과 출력층 사이에 하나만 추가함) ## dropout을 사용하면 좀더 큰 구조의 모델을 사용하더라도 과적합이 덜 발생함 -&amp;gt; dropout 사용할 땐 베이직 모델보다 노드 개수 더 늘려도 됨 def Dropout(word_num): dropout_model = tf.keras.Sequential([ tf.keras.layers.Dense(128, input_shape = (word_num,), activation = &#39;relu&#39;), tf.keras.layers.Dropout(0.3), tf.keras.layers.Dense(128, activation = &#39;relu&#39;), tf.keras.layers.Dropout(0.3), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)]) return dropout_model [ 과적합 - Batch normalization ] ## 배치 정규화를 적용하기 전 모델 정의 def generate_basic_model(): basic_model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128), tf.keras.layers.Activation(&#39;relu&#39;), tf.keras.layers.Dense(128), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)]) return basic_model ## 기본 모델 (base_model) 의 각 Dense Layer 사이에 배치 정규화 레이어를 적용한 모델을 정의 def generate_batch_norm_model(): bn_model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128), tf.keras.layers.BatchNormalization(), ## tf.keras.layers.Activation(&#39;relu&#39;), tf.keras.layers.Dense(128), tf.keras.layers.BatchNormalization(), ## tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)]) return bn_model " }, { "title": "[LG Aimers] 머신러닝 2. 분류 실습 II(Python)", "url": "/posts/TIL/", "categories": "TIL, lecture", "tags": "TIL, AI, LGAimers", "date": "2024-11-25 00:00:00 +0900", "snippet": "LG Aimers 5기 AI 실습 교육 수강 내용을 정리합니다. 모든 자료는 교육 플랫폼 앨리스로부터 제공받았습니다.[ Confusion Matrix를 통한 문자 인식률 확인하기 - 다중 클래스 분석 ] from sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_digitsfrom sklearn.svm import SVCfrom sklearn.metrics import confusion_matrixfrom sklearn.metrics import recall_scorefrom sklearn.metrics import precision_scorefrom sklearn.metrics import accuracy_score## digits 데이터 불러오기def load_data(): X, y = load_digits(return_X_y = True) train_X, test_X, train_y, test_y = train_test_split(X, y, test_size =0.2, random_state=100) return train_X, test_X, train_y, test_y## SVM 모델 정의def SVM_clf(train_X, test_X, train_y): svm = SVC() svm.fit(train_X, train_y) pred = svm.predict(test_X) return pred## 출력된 Confusion Matrix 를 통해 인덱스 3의 precision과 recall, 전체 데이터에 대한 accuracy 계산 함수 구현def cal_eval(test_y, pred): index_3_precision = precision_score(test_y==3, pred==3, None) index_3_recall = recall_score(test_y==3, pred==3, None) accuracy = accuracy_score(test_y==3, pred==3) return index_3_precision, index_3_recall, accuracydef main(): train_X, test_X, train_y, test_y = load_data() pred = SVM_clf(train_X, test_X, train_y) # confusion matrix 확인 print(&quot;Confusion matrix results :\\n\\t- row : real(test_y) 0 ~ 9 label\\n\\t- column : predicted 0 ~ 9 label\\n\\n%s\\n&quot; % confusion_matrix(test_y, pred)) index_3_precision, index_3_recall, accuracy = cal_eval(test_y, pred) print(&quot;index 3의 recall : %f&quot; % index_3_recall) print(&quot;index 3의 precision : %f&quot; % index_3_precision) print(&quot;전체 accuracy : %f&quot; % accuracy) &amp;lt;\\div&amp;gt;" }, { "title": "[LG Aimers] 머신러닝 2. 분류 실습(Python)", "url": "/posts/TIL/", "categories": "TIL, lecture", "tags": "TIL, AI, LGAimers", "date": "2024-11-24 00:00:00 +0900", "snippet": "LG Aimers 5기 AI 실습 교육 수강 내용을 정리합니다. 모든 자료는 교육 플랫폼 앨리스로부터 제공받았습니다.머신러닝 2번째 파트 분류(Classification) 모델을 파이썬으로 구현해 보았습니다. Step01. 분류 모델 정의 Step02. 분류 모델을 학습용 데이터에 맞추어 학습시킴(fit) Step03. 학습된 모델을 이용하여 테스트 데이터에 대한 예측 수행(predict)[ 1. 로지스틱 회귀 ] from sklearn.linear_model import LogisticRegressiondef main(): train_X, test_X, train_y, test_y = load_data() logistic_model = LogisticRegression() logistic_model.fit(train_X, train_y) predicted = logistic_model.predict(test_X) print(&quot;예측 결과 :&quot;, predicted[:10]) plot_logistic_regression(logistic_model, train_X, train_y) return logistic_model &amp;lt;\\div&amp;gt; [ 2. SVM ] from sklearn.svm import SVCfrom sklearn.metrics import classification_report, confusion_matrix def SVM(train_X, test_X, train_y, test_y): svm = SVC() svm.fit(train_X, train_y) pred_y = svm.predict(test_X) return pred_y def main(): train_X, test_X, train_y, test_y = load_data() pred_y = SVM(train_X, test_X, train_y, test_y) # SVM 분류 결과값 출력 print(&quot;\\nConfusion matrix : \\n&quot;,confusion_matrix(test_y,pred_y)) print(&quot;\\nReport : \\n&quot;,classification_report(test_y,pred_y)) if __name__ == &quot;__main__&quot;: main() &amp;lt;\\div&amp;gt; [ 3. 베이즈 정리 - Naive bayse ] ## &quot;확인&quot; 이라는 키워드가 등장했을 때 해당 메일이 스팸 메일인지 정상 메일인지 판별하기 위한 함수를 구현def bayes_theorem(): ## 1. P(“스팸 메일”) 의 확률: 스팸메일수/전체메일수 p_spam = 8/20 ## 2. P(“확인” | “스팸 메일”) 의 확률: 스팸메일 중 확인 키워드가 포함된 메일 수/스팸메일수 p_confirm_spam = 5/8 ## 3. P(“정상 메일”) 의 확률: 정상메일수/전체메일수 p_ham = 12/20 ## 4. P(“확인” | &quot;정상 메일&quot; ) 의 확률: 정상메일 중 확인 키워드가 포함된 메일 수/정상메일수 p_confirm_ham = 2/12 ## 5. P( &quot;스팸 메일&quot; | &quot;확인&quot; ) 의 확률: ## P(스팸 메일|확인) = P(확인|스팸 메일) * P(스팸 메일) / P(확인) p_spam_confirm = p_confirm_spam * p_spam / (7 / 20) ## 6. P( &quot;정상 메일&quot; | &quot;확인&quot; ) 의 확률: ## P(정상 메일|확인) = P(확인|정상 메일) * P(정상 메일) / P(확인) p_ham_confirm = p_confirm_ham * p_ham / (7 / 20) return p_spam_confirm, p_ham_confirmdef main(): p_spam_confirm, p_ham_confirm = bayes_theorem() print(&quot;P(spam|confirm) = &quot;,p_spam_confirm, &quot;\\nP(ham|confirm) = &quot;,p_ham_confirm, &quot;\\n&quot;) ## 두 값을 비교하여 확인 키워드가 스팸에 가까운지 정상 메일에 가까운지 확인합니다. value = [p_spam_confirm, p_ham_confirm] if p_spam_confirm &amp;gt; p_ham_confirm: print( round(value[0] * 100, 2), &quot;% 의 확률로 스팸 메일에 가깝습니다.&quot;) else : print( round(value[1] * 100, 2), &quot;% 의 확률로 일반 메일에 가깝습니다.&quot;)&#39;&#39;&#39;P(spam|confirm) = 0.7142857142857143 P(ham|confirm) = 0.2857142857142857 =&amp;gt; &#39;확인&#39; 키워드가 있을 때 스팸 메일일 확률이 0.71&#39;&#39;&#39; [ 4. 베이즈 정리 - Naive bayse(Scikit-learn) ] 가우시안 나이브 베이즈 모델은 ‘확인’키워드 유무(O or X)와 같은 이산적 데이터가 아닌 연속적인 값(예를 들어 3.14…)을 가진 데이터에 적용할 수 있다는 특징을 가지고 있음 from sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import accuracy_scoredef Gaussian_NB(train_X, test_X, train_y, test_y): model = GaussianNB() model.fit(train_X, train_y) predicted = model.predict(test_X) return predicted def main(): train_X, test_X, train_y, test_y = load_data() predicted = Gaussian_NB(train_X, test_X, train_y, test_y) print(&quot;\\nModel Accuracy : &quot;) print(accuracy_score(test_y, predicted)) [ 5. 혼동 행렬(Confusion matrix) ] True Positive: 실제 Positive인 값을 Positive라고 예측(정답) TrueNegative: 실제 Negative인 값 Negative라고 예측(정답) False Positive: 실제 Negative인 값을 Positive라고 예측(오답) False Negative: 실제 Positive인 값을 Negative라고 예측(오답) import numpy as npimport matplotlib.pyplot as pltfrom sklearn.svm import SVCfrom sklearn.metrics import confusion_matrixfrom sklearn.utils.multiclass import unique_labels# Confusion matrix 시각화를 위한 함수입니다.def plot_confusion_matrix(cm, y_true, y_pred, classes, normalize=False, cmap=plt.cm.OrRd): title = &quot;&quot; if normalize: title = &#39;Normalized confusion matrix&#39; else: title = &#39;Confusion matrix&#39; classes = classes[unique_labels(y_true, y_pred)] if normalize: # 정규화 할 때는 모든 값을 더해서 합이 1이 되도록 각 데이터를 스케일링 합니다. cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(title, &quot;:\\n&quot;, cm) fig, ax = plt.subplots() im = ax.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) ax.figure.colorbar(im, ax=ax) ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=classes, yticklabels=classes, title=title, ylabel=&#39;True label&#39;, xlabel=&#39;Predicted label&#39;) # label을 45도 회전해서 보여주도록 변경 plt.setp(ax.get_xticklabels(), rotation=45, ha=&quot;right&quot;, rotation_mode=&quot;anchor&quot;) # confusion matrix 실제 값 뿌리기 fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha=&quot;center&quot;, va=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &amp;gt; thresh else &quot;black&quot;) fig.tight_layout() plt.savefig(&#39;confusion matrix.png&#39;) elice_utils.send_image(&#39;confusion matrix.png&#39;)def main(): train_X, test_X, train_y, test_y, class_names = load_data() ## SVM 모델 생성, 학습 classifier = SVC() y_pred = classifier.fit(train_X, train_y).predict(test_X) cm = confusion_matrix(test_y, y_pred) plot_confusion_matrix(cm, test_y, y_pred, classes=class_names) ## 정규화 된 혼동 행렬 시각화 plot_confusion_matrix(cm, test_y, y_pred, classes=class_names, normalize = True) return cm [ 6. 정확도, 정밀도, 재현율 ] 정확도(accuracy): (TP+TN) / (TP+FP+FN+TN) 정밀도(precision): TP / (TP+FP) 재현율(recall): TP / (TP+FN) 데이터가 불균형하거나 다양한 상황에서 정확도만으로는 모델을 평가하는 데 한계가 있으므로 다른 지표들을 함께 사용함 import pandas as pddef main(): # 실제 값 y_true = pd.Series( [&quot;not mafia&quot;, &quot;not mafia&quot;, &quot;mafia&quot;, &quot;not mafia&quot;, &quot;mafia&quot;, &quot;not mafia&quot;, &quot;not mafia&quot;, &quot;mafia&quot;, &quot;not mafia&quot;, &quot;not mafia&quot;] ) # 예측된 값 y_pred = pd.Series( [&quot;mafia&quot;, &quot;mafia&quot;, &quot;not mafia&quot;, &quot;not mafia&quot;, &quot;mafia&quot;, &quot;not mafia&quot;, &quot;not mafia&quot;, &quot;mafia&quot;, &quot;not mafia&quot;, &quot;not mafia&quot;] ) print(&quot;1. 혼동 행렬 :\\n&quot;,pd.crosstab(y_true, y_pred, rownames=[&#39;실제&#39;], colnames=[&#39;예측&#39;], margins=True)) &quot;&quot;&quot; 1. 실행 버튼을 클릭하여 마피아(mafia)와 시민(not mafia)으로 분류된 혼동 행렬을 확인함 &quot;&quot;&quot; &quot;&quot;&quot; 2. 실행 결과값을 토대로 마피아를 제대로 분석했는 지에 대한 accuracy, precision, recall을 구함 &quot;&quot;&quot; accuracy = (2+5)/10 precision = 2/4 recall = 2/3 print(&quot;\\naccuracy : &quot;, accuracy) print(&quot;precision : &quot;, precision) print(&quot;recall : &quot;, recall) return accuracy, precision, recall " }, { "title": "24.11.18(Mon)", "url": "/posts/TIR/", "categories": "TIR", "tags": "TIR", "date": "2024-11-18 00:00:00 +0900", "snippet": "오늘 할 일 강동일자리카페(퍼스널컬러 진단) 코테 문제 풀기 포트폴리오 작성(프로젝트 1개에 대한 내용 자세히)정리 오늘은 서울시일자리카페 프로그램에 참여했다. 지난달에는 소그룹 자소서 컨설팅을 다녀왔는데 이제 막 취업 준비를 시작하는 입장에서 자기소개서를 어떤 방향으로 작성해야 하는지 감을 잡는데 도움이 되었고, 오늘 퍼스널 컬러 진단은 재미로 다녀왔지만 취업 관련한 조언을 잠깐 해주셔서 마음이 따뜻해졌다. 끝나고는 왕십리에서 친구들하고 저녁을 먹었는데 술 한잔 마시지 않고도 이렇게 즐거운 시간을 보낼 수 있음에 감사했다.퍼컬 강사님께서 강의를 마무리하며 오늘 집에 돌아가는 길에 친구, 가족에게 나의 장점을 한가지씩 들어보라고 하신 게 생각나서 그 자리에 있던 친구들과 잠깐 이 주제로 대화를 해봤다. 내가 단점이라 생각하는 부분을 친구들은 장점으로 바라보고 있어서 놀랐고 내가 생각하는 나와 남이 바라보는 나의 모습이 정말 다르다고 느꼈다. 그리고 먼저 취업을 한 친구들이 해주는 경험담을 들으면서 취준 기간동안 정말 힘들었겠단 생각이 들면서도 그 시간을 잘 이겨내고 지금의 자리에 있는 친구들이 참 건강하고 멋지다는 생각을 했다.피드백 오전 시간을 제대로 활용하지 못해서 오늘 목표한 걸 끝내지 못했다.아침에 일어나서 밥 먹고 준비하는 시간이 너무 길어서 그런 것 같다.간단히 먹고 빨리 나갈 수 있도록 미리 준비해 놔야겠다.내일 할 일 코테 문제 풀기 포트폴리오 작성(프로젝트 1개에 대한 내용 자세히)" }, { "title": "24.11.17(Sun)", "url": "/posts/TIR/", "categories": "TIR", "tags": "TIR", "date": "2024-11-17 00:00:00 +0900", "snippet": "오늘 할 일 런베뮤~! 코테 연습 실험 코드 정리 논문 작성정리 오늘은~ 엄마랑 런베뮤에서 7만원치 베이글 쇼핑하고 건대에 다녀왔다. 도산점은 웨이팅이 적다고 해서 그쪽으로 갔는데 집에서 출발하기 전에 캐치테이블 걸고 가니까 시간이 딱 맞았다. 베이글 먹고 건대로 넘어가서 오랜만에 오프라인 쇼핑도 하고 고기국수까지 먹은 알찬 하루였다. 터미널에 가서 엄마를 배웅하고 집으로 돌아와 저녁을 먹고 논문쓰러 카페에 갔다.할리스가 24시간 영업이라서 9시정도에 갔는데 일요일 밤 이 시간에 커피마시는 사람이 이렇게 많은 줄 몰랐다…. 노트북하기 좋은 자리가 있어서 왔는데 이미 꽉 찼고 카운터 앞에 콘센트 없는 자리만 비어있어서 일단 그쪽에 앉았다.. 1시간정도 지나니까 원하던 자리가 생겨서 옮겼는데 12시까지도 사람이 바글바글했다. 다들 취준생인걸까..? 모르는 사람들이지만 잠깐 동질감을 느끼기도 했다..ㅎ내일 오전 미팅이 있었는데 금요일로 미뤄졌다. 원래 오늘밤에 논문을 완성해야해서 마음이 불안하고 힘들었는데 약간의 시간이 생겨서 여유있게 작업하고 책도 읽을 수 있었다. 그래도 더 미룰 수는 없으니 목표했던 시간까지 카페에서 작업하다가 집에 왔다. 갑자기 날이 많이 쌀쌀해져서 감기에 걸릴까봐 걱정이다. 당분간 컨디션 조절에 더 신경써야겠다.내일은 오전에 일정이 취소되어 조금은 여유롭게 준비하고 오후 일정을 나가면 될 것 같다. 그래도 계획없이 움직이면 또 하루를 낭비하게 될 것 같다. 내일도 오늘 세운 계획대로 움직일 수 있게 노력하기!피드백 이번주엔 꼭!! 실험 코드 정리하기내일 할 일 강동일자리카페(퍼스널컬러 진단) 코테 문제 풀기 포트폴리오 작성(프로젝트 1개에 대한 내용 자세히)" }, { "title": "24.11.16(Sat)", "url": "/posts/TIR/", "categories": "TIR", "tags": "TIR", "date": "2024-11-16 00:00:00 +0900", "snippet": "오늘 할 일 실험 코드 정리 논문 작성정리[논문작성]코드를 공개용으로 정리해야 하는데 실험 중간중간에 하나씩 추가하다보니 큰 틀에서 바뀐 내용이 많다.수정하려면 전체 틀을 다시 구성해야 할 것 같다.논문은 지난 미팅 내용을 반영해서 수정했는데 오랜만에 글을 쓰려니까 머리가 안돌아가는 느낌이다..예전에 작성해둔 연구노트+자료 보면서 복기하는데도 꽤 걸렸다. 연구노트의 중요성을 다시 한번 깨닫게 된….ㅎ힘내서 얼른 마쳐야겠다.[카테고리명변경] 카테고리명을 TIL에서 TIR로 바꿨다. 매일 배운 내용을 정리하려고 했는데 일기형식으로 작성하는 것 같아서..ㅎㅎ하루하루 계획 세우고 지켰는지 체크하는 용으로 사용하는 게 좋겠다.피드백 오늘은 늦잠을 잤는데 저녁에 엄마가 오기로 해서 신경 쓰이지 않게 구석구석 대청소를 하다보니 공부 시작 시간이 많이 늦어졌다.역시 일찍 자고 일찍 일어나야 하루를 더 알차게 보낼 수 있는 것 같다.오늘부터 다시 일찍 취침하고 기상하는 습관을 들여야겠다.내일 할 일 런베뮤~! 코테 연습 실험 코드 정리 논문 작성" }, { "title": "[Lecture] PT면접 노하우", "url": "/posts/lecture-PT%EB%A9%B4%EC%A0%91/", "categories": "lecture", "tags": "lecture, 취업준비, 면접준비, PT면접", "date": "2024-11-12 00:00:00 +0900", "snippet": "한국고용정보원-사이버진로교육센터에서 제공하는 ‘토론면접과 프레젠테이션 면접의 실전 노하우’의 내용을 정리한 글입니다.PT면접 지원자의 전문성과 문제해결능력을 평가하기 위해 활용하는 면접 방식전문성 + 문제해결능력 + PT능력[ PT면접 유형별 구분 ] 문제해결 제시된 문제 사항을 체크한 후 관련 문제에 대한 해결 방안 제시 당사의 시장 점유율이 정체되고 있는 상황에서 당사의 시장 점유율을 높일 수 있는 방안을 제시하시오. 저출산의 시대적 흐름에서 당사가 나아가야 할 방향을 제시하시오. 우리 회사의 제품과 서비스를 활용한 사회공헌 활동을 기획하여 발표하시오. 본인이 회사 대표라면 조직 문화를 어떻게 바꾸고 싶은지를 제안하여 발표하시오. 편의점 슈퍼바이저로 일하는 중인데 점포주가 회사의 방침을 따르고 있지 않다면 본인은 해당 문제를 어떻게 해결할 것인가? 자료분석 5~10페이지 자료 제공(회사에 따라 분량은 다름) - 각종 데이터 자료 등 다양하게 제공, 관련 자료를 기반으로 하여 의사결정하는 방향으로 출제 자료를 효과적으로 활용할수록 좋은 평가 받음 시간 내 자료를 분석하기 어려운 경우가 많음 30~40분 정도 분석 후 PPT 혹은 워드, 전지에 작성을 한 다음 발표 진행 (진행 방식도 회사마다 다름) 사전준비 사전에 회사에서 주제 제시 면접 전에 발표 자료를 PPT/Word로 구성하여 사전 제출 석박사는 대부분 논문 요약본 요구 전공지식 이공계 학생들에게 주로 출제 출제된 전공지식을 통해 지원자의 전공 역량 측정 재무회계 분야에서도 전공지식 측정하는 경우가 있음 지원하는 회사의 면접 기출을 파악하여 전공 공부 필요한지 판단하여 준비 필요 [ PT면접 Tip ]- 보통 현장에서 PPT 작성하는 경우는 거의 없음- 발표 내용 작성할 종이를 제공하여 발표 시 참고할 수 있도록 함- 발표할 땐 화이트보드나 전지 제공- 보통 2-40분정도 준비 시간 주어짐- 제시된 자료 외 정보 검색 불가한 경우가 많음- 대체로 서서 진행(일부 기업에서는 앉아서 진행하기도 함)- JobErum - 면접유형별 기출질문 답변에서 [PT면접주제](https://www.joberum.com/interview/zocbo/zocbo01.asp) 확인 가능[ PT면접 평가표 항목 ] 주제 이해도 질문에 대한 주제를 명확히 이해하는 것 토론면접의 경우 주제를 완전히 이해하지 못했어도 눈치껏 커버 가능하지만 PT는 그렇지 않음(따라서 관련 지식을 꾸준히 공부해두는 게 필요함) 면접 전부터 꾸준히 직무에 연관성 있는 전공지식 준비 필요 기업마다 유형이 다르므로 희망 기업의 면접형태를 미리 확인(전공지식중심/배경지식중심/자료기반 등..) 전공지식(이공계열) 회계, 재무, 법무, 금융관련(사무직) 회사 이슈/사업 이슈/직무 관련 주제.. 하루 30분 이상 꾸준히 회사 관련 뉴스 기사, 신문 사설/컬럼 읽어두면 도움이 됨 문제해결능력 특정 현안에 대한 문제해결능력을 요구함 전문지식 + 배경지식을 기반으로 문제를 해결해야 함 기업에서 제공하는 자료를 기반으로 문제를 풀어야 함 질문을 잘 해석해서 요구하지 않은 것에 대한 답변하지 않도록 주의! 자료분석능력 기업에서 자료(수치가 있는 데이터, 문제 상황 관련 자료 등을)를 제시해주는 경우가 있음 자료를 잘 분석해서 활용하기 전달력(발표력) 다른 부분에서의 수준은 크게 차이나지 않을 수 있음 스피치&amp;amp;이미지는 매우 중요한 요소이므로 한가지 특정 주제로 대본을 만들어 수십 번 이상의 반복 연습을 통해 제대로 된 발표 준비하기 스마트폰으로 자신의 발표 모습을 촬영하여 확인하거나, 스터디를 통해 지속적으로 연습하기 예절/태도 기본적인 면접 매너 지키기(인사 안하는 경우가 많음..) 화이트보드를 이용하는 경우 보드만 바라보며 발표하거나, 준비한 대본만 보며 발표하지 않도록 주의 [ PT면접 8단계 프로세스 ]- 오프닝멘트 - 면접관의 이목을 집중시키기 위한 내용으로 시작 - ex. 안녕하십니까? 함께하면 좋은 지원자 김지연입니다.- 서론/주제 - 본론 전, 발표 주제 언급 - ex. 이번에 제가 발표할 주제는 ‘정체된 점포의 매출 확대 방안’입니다. or ~에 대해 발표를 진행하도록 하겠습니다.- (판서) - 발표할 내용을 간략하게 정리하여 화이트 보드에 작성 - 최대한 짧게, 키워드만 도식화하여 표현!(화이트보드 사용 안하는 경우도 있음) - 5분 발표라면 30초 전후로(ex. **면접관분들의 이해를 돕기 위해 잠시 판서를 하도록 하겠습니다. 기다려주셔서 감사합니다. 칠판을 주목하여 주시기 바랍니다.**)- (목차 설명) - 앞으로 전개될 내용 순서에 대해 간략히 안내(발표의 내용이 어떻게 흘러 가는지 안내하는 정도) - 발표 시간이 짧은 경우에는 목차 설명 생략!- 본론 - 핵심만 간결하게, 논리적으로 - 첫째, 둘째, ..- (요약/종합) - 본론 내용 요약 - 전체 내용을 종합하여 전달 - 시간이 촉박한 경우 생략- 클로징 멘트 - 내용 마무리 - 질의 응답!! - ex. 제가 준비한 내용은 여기까지입니다. 경청해주셔서 감사합니다. 혹시 궁금한 사항이 있으시다면 답변해 드리도록 하겠습니다.- (질의응답) - 2~3개, 많으면 5~6개 이상.. - 잘못된 점을 지적받을 수 있음 → 면접관의 의견을 반영하여 답변 수정하는 유연함 - 가벼운 인성 면접 질문을 하기도 하므로 당황하지말고 답변하기[ 태도 Tip ] 바른 자세와 태도 서있는 자세부터 평가에 반영됨 바른 자세와 자심감 있는 태도! 면접관이 좋아하는 이미지 만들기 이미지에 영향을 주는 요소: 시각(55%), 청각(38%), 말의 내용(7%).. → 비언어적인 표현(아이컨택, 제스처 등)에도 신경쓰기 효과적인 제스처 사용 과도한 제스처는 산만한 사람으로 보일 수 있음 핵심 내용을 강조하고 효과적으로 전달하는 데 사용 시선, 자세 모두 면접관에게 고정하기!! 준비한 자료를 보기 위해 시선을 아래로 두거나, 보드쪽으로 몸을 돌리는 경우가 많음 전반적인 시선과 자세를 면접관 방향으로 두기 주제에 집착하기보다는 발표능력에 집중 주제를 예측하기는 어려우므로 이전에 출제된 수준에서 필요한 자료 점검하는 것은 필요하지만, 전공지식이나 배경지식은 하루이틀만에 쌓을 수 있는 게 아님 따라서 반복 연습을 통해 발표 능력을 키우는 데 집중하기 " }, { "title": "24.11.12(Tue)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-12 00:00:00 +0900", "snippet": "오늘 할 일 알고리즘 문제 풀기(BFS) 자기소개서 작성정리 오늘은 아침에 씻는데 갑자기 어제 공부한 BFS 코드가 막 생각나더라..준비하고 나가서 바로 BFS 공부 좀 하고 자기소개서를 작성했다.피드백내일 할 일 수요관측회~~!" }, { "title": "[Lecture] 토론면접 노하우", "url": "/posts/lecture-%ED%86%A0%EB%A1%A0%EB%A9%B4%EC%A0%91/", "categories": "lecture", "tags": "lecture, 취업준비, 면접준비, 토론면접", "date": "2024-11-11 00:00:00 +0900", "snippet": "한국고용정보원-사이버진로교육센터에서 제공하는 ‘토론면접과 프레젠테이션 면접의 실전 노하우’의 내용을 정리한 글입니다.토론면접 토론면접 4~10명이 찬/반으로 나누어 2~40분정도 토론은 말만 잘하는 사람을 채용하기 위한 자리가 아님 주어진 토론 주제에 대해 논리적으로 주장할 수 있어야 하고, 다른 참여자와 상호작용이 매우 중요함!! 면접관은 토론 면접을 보면서 “각 지원자의 가치와 성향 파악” 따라서, 말하는 내용 뿐만 아니라 태도도 중요하게 생각해야 함 JobErum - 면접유형별 기출질문 답변에서 토론면접주제 확인 가능 ### [ 토론과 토의 ] 토론 찬반을 나누어 경쟁적으로 의사소통하는 과정을 관찰 면접 전에 찬성/반대/사회자를 지정하거나, 개인이 가지는 순수한 의견으로 참여 4~10명(평균 6명)씩 참여 인원수에 따라 진행 시간 차이 있지만, 보통 3-40분(최대 60분) 토론면접 종료 후 다:다 면접 진행하는 경우 있음 토의 공통의 합의점을 도출하기 위해 협력적으로 의사소통하는 과정을 관찰 → 최근에는 “토의면접”을 선호하는 추세 ### [ 주제에 따른 분류 ] 시사이슈 평소 상식적으로 알고 있어야 할 다양한 시사 상식 이슈 ex. 국민연금 개혁 광범위한 분야에서 출제되는 경향이 있으므로 평소에 신문, 사설, 최근 뉴스 등으로 사회 이슈에 대한 지식 쌓는 것이 중요 회사이슈 회사 내부 or 관련 산업 이슈, 직무에 대한 문제 제시 관련 분야의 지식이 많을수록 좋음 어려운 주제일수록 어느정도 자료를 제공해줌 Al 선도 기술을 지향하고자 하는 자사의 이미지 제고를 위한 홍보 전략 당사는 최근 a 부품을 공급하는 업체를 새롭게 알아보고 있다. 이를 위해 총 5개의 기업 중 한 기업을 선택하려고 한다. 가장 적합한 기업을 조원들과 협의하여 선정하라. 창의성 다양한 배경 지식 또는 창의적 발상과 아이디어를 갖추면 유리함 - 제시된 10가지 목록 중 무인도에 들어갈 때 반드시 필요한 것 세 가지를 협의하여 선정하기 - 고령화 사회에 있어 AI 기술로 노인들의 문제를 해결할 수 있는 아이디어 한 가지 선정하기 [ 토론면접 평가표 항목 ] 토론면접에서 평가 가능한 항목의 예시임 주도성 지원자가 몇번이나 발언 기회를 갖게 될지 생각 주도적인 발언권 확보로 적극적인 모습 보이는 것 중요 토론을 진행하는 사회자가 없더라도, 토론을 시작하는 데 도움을 주거나 방향성을 잡아가는 리더의 역할을 하면 매력적인 모습 커뮤니케이션 주제에서 벗어나는 논리를 제시하거나, 근거가 빈약한 논리 제시하면 안됨 주제에 맞는 이야기 + 주장에 적절한 이유와 근거 제시하며 말하는 습관 상대방과의 대화 흐름 파악하며 참여하기 위해 집중! 참신성 배경지식의 부족으로 같은 관점만 고집하고 새로운 관점을 전혀 제시하지 못하는 지원자가 많음(10분만 지나도 더이상 할말이 없는) 평소 독서, 신문을 통해 배경지식 기르기 특히, 회사와 연관된 이슈 충분히 파악하고 가는 게 좋음 포용력 독단적으로 자신의 생각만을 표현하거나, 다른 사람의 말을 무시, 끼어들거나 자신의 주장을 끝까지 고집하는 것은 감점 요인 팀의 목표를 우선하며 다른 팀원의 의견 존중 발언을 못하는 팀원의 답변을 유도하거나 답변을 유도하는 모습 보이기 예절/태도 지원자의 인성(깔끔한 복장, 호감가는 표정, 밝은 목소리를 통해 긍정 이미지 형성) 지원자 간 소통하다보면 집중력을 잃고 산만한 모습을 보이는 경우가 있음 면접관이 다 보고 있으므로 태도 주의하기 → 항목마다 최소 점수가 있을 것. 한 가지 항목이라도 낮은 평가 받으면 다른 항목에도 영향을 미치고, 전체 평가가 부정적으로 이어지므로 각 항목 유의하기[ Q&amp;amp;A ] 주제와 관련없는 이야기를 하는 참여자가 있을 때 → 차갑고 공격적인 어투로 상대방을 무시하는 듯이 말하지 말고, 최대한 예의를 갖추어 “네, 이야기를 잘 들었습니다. 하지만 현재 우리가 논의하는 방향이 주제와 동떨어진 듯한 느낌이 있는 것 같습니다. 동의하신다면 주제와 좀 더 일치하도록 토론을 진행하는 것이 어떨지 제안드리고 싶습니다.”와 같이 말을 해볼 것 토론을 혼자 준비하는 방법 → 토론 가능한 주제 직접 제작, 기출 문제 체크 ⇒ 하나의 주제에 대해 찬/반 입장으로 나누어 연습 특정 사안에서 다양한 관점이 존재함을 발견하고, 자신의 발언 내용을 스스로 평가함으로써 답변 오류를 발견 가능 *PREP 스피치 기법 PREP은 논리적으로 의사를 표현하는 데에 사용하는 스피치 기법으로, Point(주장), Reason(이유), Example(사례), Point(주장) 의 순서로 자신의 주장을 펼치는 방식" }, { "title": "24.11.11(Mon)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-11 00:00:00 +0900", "snippet": "오늘 할 일 알고리즘 문제 풀기(BFS) 자기소개서 작성 고용노동부 온라인교육 수강(서류전형 준비, 토론PT면접)정리[코테] BFS 기초 정리 + 문제 유형 파악부끄럽지만 BFS 코드를 만져본지가 오래 되어서 쉬운 문제 하나 제대로 풀지 못한다. 그래서 어제 그려놓은 그림과 BFS 코드를 놓고 진짜 오랜만에 클론코딩하면서 손에 익혔다. 오늘은 이해하는 데 시간이 걸려서 진도를 많이 나가지 못했는데 내일부터는 좀더 속도를 내야겠다.[온라인교육] 서류전형 준비는 익숙한 내용이었지만 토론PT면접은 아직 그 단계까지 가보지 않아서 낯설었고 주변에서도 후기를 들어본 적이 없어서 강의를 통해 많은 것을 알게 되었다. 들으면서 정리한 내용은 따로 하나 올려두어야겠다.피드백 외출할 때마다 노트북을 항상 가지고 다니니까 어깨가 아파서 책을 놓고 다니게 된다. 내일은 도서관으로 가서 책도 좀 읽고 오늘보다 더 알찬 하루를 보내야겠다.내일 할 일 고용노동부 온라인교육 수강(토론PT면접) 자기소개서 작성 알고리즘 문제 풀기(BFS)" }, { "title": "24.11.10(Sun)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-10 00:00:00 +0900", "snippet": "오늘 할 일 자기소개서 작성 알고리즘 문제 풀기 -&amp;gt; 개념 정리(BFS)정리[코테] 면접까지 가려면 코테 공부를 미리 해둬야겠단 생각을 항상 하지만 당장 급하지 않으니 다른 일들보다 우선순위가 낮아진다..인적성 하나 마치고 잠깐 여유가 생겨서 다음주 계획을 세우는 김에 오랜만에(?) 알고리즘 공부를 했다.BFS 기초부터 공부하려고 블로그에 잘 정리된 코드들 보면서 손으로 그림 그려가며 정리해봤는데 이렇게 각 잡고 알고리즘 공부하는 게 넘 오랜만이라 어렵게 느껴졌다.학부 때 자바로 그래프 구현했던 게 벌써 한 5년 전….? 시간이 이렇게 빠르다니..석사 때도 알고리즘을 수강하긴 했지만 실습은 따로 하지 않아서 결국 다시 공부해야 한다.그래도 완전 새로운 개념들이 아니라서 이해하는 데는 오래 걸리지 않는 것 같다. 코드로 구현하는 게 어려울 뿐..!오늘부터는 간단한 문제라도 최소 하루에 한문제 이상 풀어야겠다.피드백 코테 실력은 하루 아침에 느는 게 아니니까 매일 꾸준히 연습해야겠다.낮에는 회사에서 일 하거나 부트캠프에 참여하거나.. 개인 공부를 열심히 하면서도 취업 준비를 하는 사람들이 있다는 걸 잊지 말자.모두에게 똑같이 주어진 시간을 잘 활용하자!내일 할 일 알고리즘 문제 풀기(BFS) 자기소개서 작성" }, { "title": "24.11.09(Sat)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-09 00:00:00 +0900", "snippet": "오늘 할 일 인적성검사 응시(화이팅..!) 인적성검사 후기 정리 바깥 공기 쐬기~정리[인적성검사후기] 적성검사 후 인성검사였는데 적성은 6개, 인성은 2개 파트로 이루어져 있었다. 적성에서 한 항목당 주어지는 시간은 6~9분정도..?항목별로 문제 수와 시간이 달랐는데 문제만 보고 푸느라 전체 몇 문제였는지까지는 기억나지 않고, 가장 자신있었던(?) 추리 파트가 응시 환경 오류로 인해 아예 문제조차 구경하지 못한 게 아쉽다.(전체에게 발생한 문제라서 평가에 반영하지 않는다고 함)근데 오픈채팅 보니까 다른 분들도 파트6이 제일 자신있었다고 하셔서 오히려 다행이라 생각함..ㅎㅎ 역시 세상엔 똑똑한 사람이 많아..파트1은 대응되는 단어 선택이었는데 은근 어려웠다. 다른 분들 후기에서처럼 처음 보는 단어들도 많고.. 예시처럼 명확한 관계 파악이 어려운 것들이 많아서 느낌으로 찍기도 어려웠다.수리는 반절정도 풀었나..? 수영장 깊이 구하는 문제가 쉬워보였는데 암산하려니까 반복해서 계산하느라 좀 걸렸다.. 그거 풀고 시간이 끝나서 다음 파트로 넘어간 듯 ㅜㅜ대망의 전개도는 진짜 대박 어려웠음!!!2번까지 풀고 쉬운 도형 먼저 풀어야겠다 싶어서 바로 16번인가..? 제일 끝 문제로 넘어갔는데 풀 수 있을 것 같아서 고민하다보니 시간 끝나버림….허무 그자체.. 얼마나 지났지? 싶어서 시간을 본 순간 5초인가 남아있어서 급하게 바로 앞 문제를 클릭하고 하나 찍은 것 같은데 앞 문제는 정육면체였다..ㅜㅜ 쉬운거 먼저 골라서 풀려고 뒤로 넘어간거였는데 결국 더 쉬운 문제는 구경도 못하고 패스한 ㅜㅜ 이 파트가 9분인가? 다른 파트에 비해 시간이 길었던 걸로 기억하는데 제일 못 푼 게 아쉬웠다.문자 찾기는 2개 유형이었는데 첫번째는 다 풀었고 두번째 풀던 중에 갑자기 종료됨.. 문제에 초집중하다보니 시간 체크할 여유가 없었다. 근데 시간 조절하면서 풀 파트는 아닌 듯.. 어차피 한 문제 보기 시작하면 그냥 푸는 게 나으니까 그냥 바로바로 다음 문제 넘기면서 최대한 많이 푸는 게 나은 것 같다.(내생각)수열은 처음에 예시랑 완전 비슷한 패턴으로 나와서 왜 이렇게 쉽지? 했는데 중간쯤부터 패턴이 하나도 안보였다. 뒤에서부터 다시 앞으로 돌아오면서 풀었는데 패턴이 딱 읽히는 게 없어서 어떤거 하나 잡고 풀다가 끝난 듯파트 6으로 넘어갈 때 갑자기 접속이 안되어서 당황했다. 집에 인터넷이 가끔 끊기는데 하필 지금인가..? 싶다가 다른 페이지는 잘 뜨는 걸 보고 나만의 문제는 아니라고 생각했다.침착하게 문의넣고 기다리는 와중에 계속해서 시간은 가고..(접속 페이지에서 시간 줄어드는 건 왜 잘보이는지..ㅜㅜ)기다리다가 인성검사 시간이 되어서 넘어갔는데 촉박한 와중에 공지사항 알림 소리가 났다.보고 싶은데 인성검사 페이지당 시간이 촉박해서 볼 수가 없어….ㅠㅠ 신경쓰면서 풀다가 빨리 체크한 페이지가 있어서 얼른 공지보고 다시 돌아와서 풀었다.유형2에서 모의테스트 봤을때보단 일관적으로 체크하려고 노력했는데 뒤로 갈수록 앞에서 본거랑 비슷한 문항이 계속 나와서 이 중에 내가 뭘 가깝다로 체크했지..? 점점 헷갈렸다. 분명 내가 평소에 더 중요하다고 생각하는 것도 문항에 따라 약간씩 뉘앙스가 다르니까.. 똑같이 각각 키워드 A, B에 대한 문항이더라도 앞에서는 A가 더 중요했는데 뒤에서는 B가 더 중요해지는 상황..내 성향을 아주아주 구체적으로 정리해놔야 흔들리지 않을 것 같다.모의테스트에서 낮게 나왔던 부분을 신경쓰면서 체크하려고 노력했는데 뭔가..더 어려웠다.그리고 인성검사 문제는 하루종일 생각이 나더라..ㅎ 생활하다가 문득 한 문항씩 떠올라서 오빠랑 길 걷다가도 근데 이런 성격이 더 좋은걸까? 이럴 땐 뭐가 더 중요하다고 생각해??.. 갑자기 N이 되어서 막 상상하다가 가치관 토론해버림. 인성검사 본 날 친구 만나면 하루종일 떠들 수 있을거같다 ㅎㅎㅎ(후기 정리) 다음 인적성은 더 잘 볼 수 있을 것 같다. 특히 필기가 가능한 경우엔..!두달전쯤 skct 볼 기회가 생겨서 잠깐 준비했었는데 그땐 너무 어렵다고 느꼈던 문제들을 필기없이 풀어보니까 ‘계산기와 메모장이 있다면 난 뭐든 풀 수 있어’가 되어버림 물론 시간은 부족하겠지만!연습하면 된다는 걸 다시 한번 알게 되었다.셤 끝나고 잠깐 쉬다가 문정역에 있는 더플레이스에 다녀왔다.사진에서 보던 피자위에 핑크색이 잠봉인 줄 알았는데 모르타델라 라는 햄이었다. 꿀 찍어먹으니까 진짜 맛있음!도우도 맛있고 끝까지 치즈가 가득 차있어서 진짜 맛있는 피자였다.파스타랑 문어 튀김도 굳~배부르게 밥 먹고 지쿠자전거 빌려서 생태화공원 찍고 달렸는데 반납하러 잠실나루까지 가니까 거의 1시간 걸렸다. 12km..!자전거 타기엔 추운 날씨였지만 달리니까 열 나서 괜찮았다. 오랜만에 재밌는 하루였음~~ 하지만 이제부터 장갑은 필수이다.피드백 내일부터 다시 화이팅!내일 할 일 자기소개서 작성 알고리즘 문제 풀기" }, { "title": "24.11.08(Fri)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-08 00:00:00 +0900", "snippet": "오늘 할 일 적성검사 준비(도형, 확률 공부 &amp;amp; 모의고사) 인성검사 준비(모의테스트-잡플랫)정리[적성검사] 소금물이나 수열, 명제, 언어추리..?는 이제 조금 풀만하다 느껴지는데 분수 암산은 정말 너무 어렵다..ㅜㅜ다른 분들은 어떻게 공부하는지 궁금하다. 암산 잘하면 매우매우 유리할 듯..!그리고 도형은 봐도봐도[인성검사] 인성을 공부한다는 게 조금 웃기지만,, 이전에 모의테스트를 보면서 여러 가치들 중에 내가 우선시하는 게 어떤건지를 정하고 일관성 있게 답변하는 건 연습이 필요하다고 느꼈기 때문에 오늘은 잡플랫 유료검사를 응시했다.응답 신뢰도는 양호가 떴지만 사회적 바람직성이 높게 나와서 실전에서는 너무 극단적인 값은 선택하지 않는 게 좋겠다.결과를 보고 나서는 풀면서 애매모호했던 문항들을 어떻게 답변하는 게 베스트일까 고민하다가 GPT한테 물어봤다.실제 기업에서 평가하는 방식은 다를 수 있지만 혼자서는 미처 생각하지 못한 부분들을 gpt가 짚어주는 느낌이었다. (특히 가깝다/멀다 문항에서 어떤 걸 위주로 선택하는 게 좋을지) 내일까지 컨디션 관리 잘하기!피드백 인적성 검사 준비는 한번 해봤으니 내일 결과와 상관없이 다음 시험은 더 요령있게 공부할 수 있을 것 같다.이번주에는 한가지에 너무 힘을 쏟은 것 같은데 주말동안 다시 자기소개서 작성, 포트폴리오 수정 작업하고 다음주에는 논문 작업 위주로 진행해야겠다!내일 할 일 인적성검사 응시(화이팅..!) 인적성검사 후기 정리 바깥 공기 쐬기~" }, { "title": "24.11.07(Thu)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-07 00:00:00 +0900", "snippet": "오늘 할 일 적성검사 준비(SKCT 응시, 시간 제한두고) -&amp;gt; 24’상 인성검사 준비(인재상 파악, 모의테스트 응시)정리[적성검사] 확률 파트, 도형(전개도) 공부 필요함![인성검사] 오늘은 인성검사 위주로 준비하려 했는데 아무것도 모르는 상태에서 유료인 잡플랫을 응시해보는 건 도움이 되지 않을 것 같았다. 그래서 이것저것 찾아보다가 중소벤처기업진흥공단에서 무료 인성검사가 가능한 것을 알게 되었다.평소에 나라는 사람이 창의성이 뛰어나다고 생각하지 않아서 소극적으로 답변했더니 점수가 낮게 나온 것 같다.(+ 아래 상세한 설명이 너무 정확해서 소름..ㅎ)새로운 걸 시도하려고는 하지만 다방면에서 적극적이지는 않은 게 지금 내 성향인가보다.그리고 도전 정신, 팀워크, 경쟁..같은 키워드 각각에 대해서는 주관이 뚜렷하지만 이들 중에 우선순위가 어떻게 되는지는 생각해보지 않아서 가깝다/멀다 같은 문제에서 일관성이 떨어질 것 같다. 남은 시간동안 이 부분을 생각해 봐야겠다.인성 검사를 준비하면서 이런 부분을 고민하게 되네 ㅎㅎ 단순히 통과하기 위해 공부를 한다기보다는 이걸 통해서 나라는 사람을 돌아보게 되는 것 같다. 좋은 듯!피드백 어제보단 나았지만 만족스럽진 않은 하루. 내일은 더 부지런히 움직여야겠다.내일 할 일 적성검사 준비(도형, 확률 공부 &amp;amp; 모의고사) 인성검사 준비(모의테스트-잡플랫)" }, { "title": "LG Aimers 5기 멘토링데이", "url": "/posts/aimers/", "categories": "lecture", "tags": "LG, Aimers, AI, LLM", "date": "2024-11-06 00:00:00 +0900", "snippet": "지난 여름에 참여한 LG Aimers 5기(24.07.01~24.09.29)를 대상으로 멘토링데이 프로그램에 참여할 기회가 생겨서 마곡에 있는 LG 사이언스파크에 다녀왔다.사전에 주제에 대한 안내가 없어서 이전 기수 후기를 몇개 보고 갔는데, 이 날은 LG AI연구원에서 개발한 생성형 AI ‘EXAONE(Expert AI)‘과 관련한 모델 연구와 ChatEXAONE 서비스에 대한 강의를 들을 수 있었다.LG AI연구원 소개 EXAONE Lab 리더 이진식 연구원님[ 주요 연구 분야 ] Fundamental Research Data Intelligence Materials Intelligence Computer Vision Natural Language Processing, Large Language Model Multimodal[ 주요 AI 기술 상용화 사례 ] 고객상담 통화음성을 실시간으로 분석하여 고객상담 업무효율 개선 상담 내용에 대해 실시간으로 커뮤니케이션해서 레퍼런스가 되는 문서를 띄워서 원활한 상담이 되도록 상담 내용 요약 &amp;amp; 메모 → 이후 상담 시 참고 EXAONE 생성이미지 기반 디자인 도안활용 타투프린터 IMPRINTU(임프린투) 출시 대량의 타투 도안을 효과적으로 생성 비전검사 AI 기술을 활용한 스마트 팩토리 시스템 구축 공장의 불량 부품 사전 예측, 검사 무인화 LG-QRAFT AI EFT 상품 개발, 미국 뉴욕증권거래소에 상장 어떻게 포트폴리오를 짜면 수익을 극대화할 수 있을지 [ EXAONE 소개 ] LG의 초거대 멀티모달 AI 언어 및 시각 정보 이해를 바탕으로 창의적인 생성 기능을 갖추었으며, ‘모두를 위한 전문가 AI’를 지향함 효율성 / 전문성 / 신뢰성 측면에서 이점 존재 플랫폼 종류 EXAONE Universe: 전문가용 대화형 AI 플랫폼 EXAONE Discovery: 신소재/신물질/신약 개발 플랫폼 EXAONE Atelier: 창의적인 발상을 돕는 멀티모달 AI 플랫폼 [ LG AI 윤리원칙 ] 무작정 데이터를 수집하지 않고, 윤리원칙에 기반하여 편향되지 않은 공정한 AI 만들기 위한 노력EXAONE 3.0 Language Model EXAONE Lab 이해주 연구원님alignment squad.[ Benchmarks ] 한국어 관련 벤치마크가 없음 MT-Bench(영어 chat) -&amp;gt; KoMT-Bench(한국어 chat) 개발 Coding Reasoning General…다양한 측면에서 중간 이상의 성능이 파트에서는 EXAONE 성능 평가를 위주로 설명해 주셨는데, LLM 분야를 잘 모르는 상태에서 들으니까 약간 어렵게 느껴졌다.예전에 학부 프로젝트를 진행하면서 뉴스 기사 요약 모델을 평가하는 게 어려웠는데 요즘은 그때보다 LLM 평가 방법이 더 다양해지고 고도화된 것 같았다.주로 사용하는 벤치마크 데이터셋이 무엇인지, 성능 평가 방법은 어떤 게 있는지를 얕게나마 알게 되었다.ChatEXAONE: Enterprise AI Agent Language Lab LLM-Agent Squad 리더 송호성 연구원님ChatEXAONE품질/기획/마케팅/R&amp;amp;D..부서 다양함생산성 측면에서 도움을 주는 서비스[ How it works ]user history는 어떻게 DB 구성할지.. 여기서 새로 알게 된 점 수학 문제 풀 때, 사람이 풀면 간단하지만 LLM은 할루시네이션이 생길수도 있고,,다양한 문제로 인해 코딩으로 푸는 방식 사용하고 있음Q) 문제가 되는 발언 어떻게 처리하는지? A) red teaming..? 전문적으로 문제되는 부분 체크하는 사람 + 윤리위원회 운영 + 외부 전문업체 활용Q) 하나의 EXAONE에 대해 아래의 여러 카테고리가 존재하는지 / 각 EXAONE을 다르게 학습했는지 A) 둘다 아님. 자세히 말할 수 없지만 기본적으로 multi-task instruct tune? 되어있음.Q) 챗봇 서비스에서 답변 생성 시간 오래 걸리는 문제 어떻게 해결하는지 A) 모든 chain flow 상에 bottle nack 걸리는 부분 파악함 → 각 step별로 쪼개서 환경을 잘못 구축했는지, .. 어떤 이유인지 파악하는 게 제일 기본적 → 익숙해지면 결과물을 보면 어느쪽에 문제가 있는지 알게 됨. 그럼 그 부분 위주로 살펴봄Q) 여행 일정 짜는 챗봇 추론 속도 문제(항공권, 숙박, 교통 등 여러개를 서치해야 하는 경우, 시간이 오래 걸림) 해결 방법? A) 병렬. 비동기 처리가 당연함강의를 들으면서 약간 흥미로웠던 부분 위주로 정리한 것..ㅎㅎ 중간에 생략된 내용이 많지만 나중에 혹시라도 LG에 지원하게 된다면 참고해야겠다." }, { "title": "24.11.06(Wed)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-06 00:00:00 +0900", "snippet": "오늘 할 일 Aimers 멘토링데이 참여 연구미팅 원티드 AI 챌린지 듣기 적성검사 준비(수열, 전개도, 확률 파트 중심)정리 Aimers 멘토링데이 내용 정리 글 작성 완료. 마곡나루 출퇴근 나쁘지 않을지도..?? 급행타니까 1시간정도 걸린다 적성검사 준비는 여전히 어렵다.피드백 내일은 더 집중해서 할일 끝내기!내일 할 일 적성검사 준비(SKCT 응시, 시간 제한두고) 인성검사 준비(인재상 파악, 모의테스트 응시)" }, { "title": "24.11.05(Tue)", "url": "/posts/TIL/", "categories": "TIR", "tags": "TIR", "date": "2024-11-05 00:00:00 +0900", "snippet": "TIL 글 작성 방식을 바꾸기로 했다. 강의를 듣거나 공부를 하면서 내가 배운 내용을 정리하는 게 목적이었는데,, 블로그 글을 작성하다보면 어느새 강의자료를 그대로 타이핑하게 되는 것 같다. 오늘 공부한 내용은 어느 정도 머리에 정리되었으니 주요 키워드를 위주로 어떤 걸 공부했는지 간단하게, 그리고 오늘/내일의 to-do-list를 적으면서 하나씩 달성하는 걸 목표로 해야겠다.오늘 할 일 실험 코드 정리 논문 작성 적성검사 준비이슈 [논문] 요즘 할 일이 다양(?)하다보니 연구에 몰입하는 시간이 매우..줄어들었다. 논문을 얼른 완성해야 하는데 시간을 너무 끌어서인지 예전 실험 내용이 가물가물하다. 다음주 중에 이틀정도는 연구노트와 실험 코드를 정리하고 미팅 내용 반영해서 수정해야겠다. [적성검사] 이번에 보는 시험은 긴 지문을 읽고 푸는 유형은 안나온다고 해서 다행이지만,, 수식을 눈으로만 계산하는 게 연습하면 가능해질지 모르겠다. 특히 거속시..소금물.. 그리고 명제나 참거짓 문제도 손으로 쓰면 쉬운데 눈으로는 너무 어렵다🥲 이것도 계속 하다보면 실력이 늘겠지..? 수열 문제는 풀다보니 처음보다는 약간 패턴이 보이는 듯!피드백 요즘 새로운 습관을 형성하려고 노력하지만 몸이 따라주지 않는 느낌이다..의지력 때문인가 싶었지만 진짜로 몸이 너무 피곤하다. 왜 이러지???그리고 하루중에 낭비하는 시간이 너무 많다. 아침에 일어나서 생각 금지-바로 준비하고 일단 외출하기, 밥 먹으면서 다른 일 하지 않기. 우선 이런 작은 것들을 실천해야겠다.내일 할 일 Aimers 멘토링데이 참여 연구미팅 원티드 AI 챌린지 듣기 적성검사 준비" }, { "title": "[TIL] 딥러닝 3. RNN 실습(Python)", "url": "/posts/TIL-%EB%94%A5%EB%9F%AC%EB%8B%9D3/", "categories": "TIL", "tags": "TIL, AI", "date": "2024-11-02 00:00:00 +0900", "snippet": "LG Aimers 5기 AI 실습 교육 수강 내용을 정리합니다. 모든 자료는 교육 플랫폼 앨리스로부터 제공받았습니다.RNN(Recurrent Neural Network)RNN은 시계열 데이터와 같은 순차 데이터 처리를 위한 대표적인 딥러닝 모델지난 시간 학습한 RNN을 파이썬으로 구현해 보았습니다.[ Vanilla RNN 코드 ] import osos.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39; ## 텐서플로우 간단한 경고 뜨지 않도록 설정import tensorflow as tffrom tensorflow.keras import layers, Sequentialdef build_model1(): &#39;&#39;&#39; layers.Embedding - 전체 단어 개수: 10개 - 벡터 길이: 5 layers.SimpleRNN - hidden state의 크기: 3 &#39;&#39;&#39; model = Sequential() model.add(layers.Embedding(10, 5)) ## input_dim, output_dim model.add(layers.SimpleRNN(3)) ## hidden_state(=vector) 크기 return modeldef build_model2(): &#39;&#39;&#39; layers.Embedding - 전체 단어 개수: 256개 - 벡터 길이: 100 layers.SimpleRNN - hidden state의 크기: 20 layers.Dense - 노드 개수: 10 - 활성화 함수: softmax &#39;&#39;&#39; model = Sequential() model.add(layers.Embedding(256, 100)) model.add(layers.SimpleRNN(20)) model.add(layers.Dense(10, activation = &#39;softmax&#39;)) ## RNN으로 분류하기 위해 Dense layer 추가 return model def main(): model1 = build_model1() print(&quot;=&quot; * 20, &quot;첫번째 모델&quot;, &quot;=&quot; * 20) model1.summary() ## 모델 성능에 도움이 되는 벡터로 임베딩되고, 그 벡터로 학습함 print() model2 = build_model2() print(&quot;=&quot; * 20, &quot;두번째 모델&quot;, &quot;=&quot; * 20) model2.summary()if __name__ == &quot;__main__&quot;: main() [ Vanilla RNN으로 IMDb 데이터 학습하기 ] Dataset : IMDb 영화별 리뷰에 대한 긍/부정 데이터 Task : 감성 분석(Sentimental Analysis) ```pythonimport osos.environ[‘TF_CPP_MIN_LOG_LEVEL’] = ‘2’ import tensorflow as tffrom tensorflow.keras import layers, Sequentialfrom tensorflow.keras.optimizers import Adamfrom tensorflow.keras.datasets import imdbfrom tensorflow.keras.preprocessing.sequence import pad_sequences def load_data(num_words, max_len): # imdb 데이터셋을 불러옵니다. # 데이터셋에서 단어는 num_words 개를 가져옵니다. (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words) # 단어 개수가 다른 문장들을 Padding을 추가하여# 단어가 가장 많은 문장의 단어 개수로 통일합니다.X_train = pad_sequences(X_train, maxlen=max_len)X_test = pad_sequences(X_test, maxlen=max_len)return X_train, X_test, y_train, y_test def build_rnn_model(num_words, embedding_len): model = Sequential() model.add(layers.Embedding(num_words, embedding_len))model.add(layers.SimpleRNN(16))model.add(layers.Dense(1, activation=&quot;sigmoid&quot;)) # 분류# sigmoid를 통해 도출된 0~1 사이의 값(=긍정일 확률)return model def main(model=None, epochs=5): # IMDb 데이터셋에서 가져올 단어의 개수 num_words = 6000 # 각 문장이 가질 수 있는 최대 단어 개수max_len = 130# 임베딩 된 벡터의 길이embedding_len = 100# IMDb 데이터셋을 불러옵니다.X_train, X_test, y_train, y_test = load_data(num_words, max_len)if model is None: model = build_rnn_model(num_words, embedding_len)optimizer = Adam(learning_rate = 0.001)model.compile(optimizer = optimizer, loss = &quot;binary_crossentropy&quot;, metrics = [&quot;accuracy&quot;])hist = model.fit(X_train, y_train, epochs=epochs, batch_size = 100, validation_split = 0.2, shuffle = True, verbose = 2)test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)print()print(&quot;테스트 Loss: {:.5f}, 테스트 정확도: {:.3f}%&quot;.format(test_loss, test_acc * 100))return optimizer, hist if name==”main”: main()" }, { "title": "[TIL] 딥러닝 3. RNN", "url": "/posts/TIL-%EB%94%A5%EB%9F%AC%EB%8B%9D3/", "categories": "TIL", "tags": "TIL, AI, RNN", "date": "2024-10-28 00:00:00 +0900", "snippet": "LG Aimers 5기 AI 실습 교육 수강 내용을 정리합니다. 모든 자료는 교육 플랫폼 앨리스로부터 제공받았습니다.RNN(Recurrent Neural Network)RNN은 시계열 데이터와 같은 순차 데이터 처리를 위한 대표적인 딥러닝 모델1. 순차 데이터란?순서(Order) 를 가지고 나타나는 데이터데이터 내 각 개체 간 순서가 중요함 시계열 데이터(Time-Series) 일정한 시간 간격을 가지고 얻어낸 데이터 예. 연도별 평균 기온, 시간별 주식 가격 등 자연어 데이터(Natural Language) 인류가 말하는 언어 주로 문장 내에서 단어가 등장하는 순서에 주목함 활용 분야 경향성 파악 ex) 주가 예측, 기온 예측, 강수량 예측 등 음악 장르 분석(오디오 파일 = 시계열) 음성 인식(음성에 포함된 단어, 소리 추출) 번역기 챗봇 2. Recurrent Neural Network[ 등장 배경 ]CNN에서 사용하는 Fully-connected Layer는 입/출력 노드 개수가 정해져 있음 → 순차 데이터는 하나의 데이터를 이루는 개체 수가 다를 수 있음(ex. 서로 다른 개수의 단어로 구성된 문장들) → FC Layer 사용 불가 &amp;amp;Rightarrow; 순차 데이터 처리를 위한 모델 등장[ 핵심 구성 요소 ] Hidden State : 순환 구조를 구현하는 장치[ 입력 데이터 구조 ] 각 xt의 의미 시계열 데이터 : 일정 시간 간격으로 나눠진 데이터에서 특정 시점의 값 자연어 데이터 : 문장 내의 각 단어 시계열 데이터 벡터 변환 입력 데이터의 각 xt는 벡터 형태 각 데이터를 이루는 Feature 값들을 원소로 벡터 변환 자연어 데이터 벡터 변환 임베딩(Embedding) 기법 : 각 단어들을 숫자 벡터로 변환 One-hot encoding Word2Vec 3. Vanilla RNN 가장 간단한 형태의 RNN 모델을 의미함(Simple RNN) 내부에 3개의 FC Layer로 구성 𝑊ℎℎ: hidden state(𝒉𝒕−𝟏)를 변환하는 Layer의 가중치 행렬 𝑊𝑥ℎ: 한 시점의 입력값(𝒙𝒕) 을 변환하는 Layer의 가중치 행렬 𝑊ℎ𝑦: 한 시점의 출력값 (𝒚𝒕)을 변환하는 Layer의 가중치 행렬 [ 연산 과정 ] 모델에 들어오는 각 시점의 데이터 xt마다 hidden state, yt 를 반복적으로 계산 (실제로 사용된 RNN 모델은 1개이지만 순환 구조를 나타내기 위해 그림상 옆으로 펼쳐놓은 것) 𝒉𝒕 계산 현재 입력값(𝒙𝒕)에 대한 새로운 hidden state 계산 ht = tanh(ht-1Whh + xtWxh) (tanh는 tangent hyperbolic 함수) y𝒕 계산 현재 입력값(𝒙𝒕)에 대한 새로운 출력값 계산 𝒚𝒕 = 𝑊ℎ𝑦Ht (이때 앞에서 계산한 hidden state를 이용) [ 의의 ] Hidden state의 의미 특정 시점 t까지 들어온 입력값들의 상관관계나 경향성 정보를 압축해서 저장 → 모델이 내부적으로 계속 업데이트 하는 값으로, 일종의 메모리로 볼 수 있음 Parameter sharing Hidden state와 출력값 계산을 위한 FC Layer를 모든 시점의 입력값이 재사용함 3개의 FC Layer가 모델 파라미터의 전부인 구조 [ 종류 ] many-to-one : 여러 시점의 입력값으로 1개의 출력값 도출 many-to-many : 여러 시점의 입력값, 여러 시점의 출력값(입/출력 개수는 같을수도, 다를수도 있음) encoder-decoder : 입력값을 받아 특정 hidden state로 인코딩 후, hidden state로부터 새로운 출력값을 만드는 구조[ 문제점 ] 시간 순서에 따라 생성되는 출력값 각 시점의 출력값과 실제값을 비교하여 손실(loss)값을 계산함 역전파 알고리즘이 시간에 따라 작동 → BPTT; Back-propagation Through Time 입력값의 길이가 길어질수록 초기 입력값과 나중 출력값 사이에 전파되는 기울기 값이 매우 작아질 가능성이 높음 → 기울기 소실 문제(Vanishing Gradient Problem) 발생 가능(=장기 의존성 다루기가 어려움)" }, { "title": "[논문리뷰] SAP Signavio Academic Models: A Large Process Model Dataset(2022)", "url": "/posts/paper_pm_1/", "categories": "paper_review", "tags": "paper-review, process-mining, process-model, business-process-model, SAP, BPM, BPMN, process-model-collection", "date": "2023-04-13 00:00:00 +0900", "snippet": " Sola, D., Warmuth, C., Schäfer, B., Badakhshan, P., Rehse, J. R., &amp;amp; Kampik, T. (2023, March). SAP Signavio Academic Models: A Large Process Model Dataset. In Process Mining Workshops: ICPM 2022 International Workshops, Bozen-Bolzano, Italy, October 23–28, 2022, Revised Selected Papers (pp. 453-465). Cham: Springer Nature Switzerland. 논문 다운로드(arxiv) SAP-SAM Github SAP-SAM 데이터셋#[ 1. Introduction ] 프로세스 모델은 조직이 운영하는 방식을 나타내며, BPM 생명 주기에 따라 프로세스를 이해하고 분석, 재설계, 자동화하기 위한 기반임 대부분의 조직은 프로세스 모델의 large repository를 보유하고 있지만, 일반 연구원들은 이러한 모델에 접근할 수 없음(조직 내부의 민감 정보가 포함될 수 있으므로 대부분 비공개) 본 논문의 목적 SAP-SAM(SAP-Signavio Academic Models)와 model collection(대규모의 프로세스, 비즈니스 모델로 구성된) 제공 관련 데이터셋의 기본 개요와 출처, 구조(structure) 제공 SAP-SAM의 selected properties &amp;amp; use cases 제시 작업 방법에 대한 권장 사항 &amp;amp; 데이터셋의 한계 설명 [ 2. Related Datasets ] SAP-SAM에 비해 기존 process model collection의 규모는 작은 편 hdBPMN [21] : 704개의 BPMN 모델 포함 RePROSory [5] : 700개의 모델 포함[ 3. Origins and Structure of SAP-SAM ] SAP-SAM에는 2011~2021년까지 SaaS(Software-as-a-Service) 플랫폼을 사용해 생성된 1,021,471개의 프로세스 및 비즈니스 모델이 포함됨 대부분의 모델은 비즈니스 프로세스 Model과 Notation(BPMN 2.0) 존재 SAP-SAI(SAP-Sagnavio Academic Institute)를 사용하면 researcher, teacher, student가 비즈니스 프로세스 모델을 생성, 분석, 실행 할 수 있음 SAP-SAI은 비상업적 연구, 교육에서만 사용할 수 있도록 제한되며 등록 시 자신이 만든 모델을 연구 목적으로 사용할 수 있음에 동의함 민감 정보와 관련하여 anonymization script가 실행되어 이메일 주소, 학생 등록 번호와 같은 정보는 후처리됨 결과적으로, SAP-SAM 모델은 2011.07~2021.09까지 72,996명의 사용자에 의해 생성됨 BPMN 모델은 BPMN 2.0 standard를 준수하며 개별 모델을 BPMN 2.0 XML로 변환 가능 데이터셋에 포함된 모델 BusinessProcessModelandNotation(BPMN) : BPMN is a standardized nota tion for modeling business processes [15]. SAP-SAM distinguishes between BPMN process models, collaboration models, and choreography models, and among BPMN process models between BPMN 1.1 and BPMN 2.0 models. Decision Model and Notation (DMN) : DMN is a standardized notation for modeling business decisions, complementing BPMN [17]. Case Management Model and Notation (CMMN) : CMMN is an attempt to supplement BPMN and DMN with a notation that focuses on agility and autonomy [16]. Event-driven Process Chain (EPC) : EPC [22] is a process modeling notation that enjoyed substantial popularity before the advent of BPMN. Unified Modeling Language (UML) : UML is a modeling language used to describe software (and other) systems. It is subdivided into class and use case diagrams. Value Chain : A value chain is an informal notation for sketching high-level end-to-end processes and process frameworks. ArchiMate : ArchiMate is a notation for the integrated modeling of informa tion technology and business perspectives on large organizations [13]. Organization Chart : Organization charts are tree-like models of organiza tional hierarchies. Fundamental Modeling Concepts (FMC) Block Diagram : FMC block dia grams support the modeling of software and IT system architectures. (Colored) Petri Net : Petri nets [18] are a popular mathematical modeling language for distributed systems and a crucial preliminary for many formal foundations of BPM. In SAP-SAM, colored Petri nets [12] are considered a separate notation. Journey Map : Journey maps model the customer’s perspective on an organi zation’s business processes. Yet Another Workflow Language (YAWL) : YAWL is a language for modeling the control flow logic of workflows [26]. jBPM : jBPM models allowed for the visualization of business process models that could be executed by the jBPM business process execution engine before the BPMN 2.0 XML serialization format existed. However, recent versions of jBPM rely on BPMN 2.0-based models. Process Documentation Template : Process documentation templates support the generation of comprehensive PDF-based process documentation reports. These templates are technically a model notation, although they may practi cally be considered a reporting tool instead. XForms : XForms is a (dated) standard for modeling form-based graphical user interfaces [2]. Chen Notation : Chen notation diagrams [3] allow for the creation of entity- relationship models. [ 4. Properties of SAP-SAM ] properties를 검증하기 위한 소스 코드(github) Modeling Notations → 서로 다른 notation으로 표현된 모델의 수 &amp;amp; percentage 100개 미만의 모델에 사용되는 notation Process Documentation Template(86 models) jBPM4(76 models) XForms(20 models) Chen Notation(3 models) ⇒ 주로 사용되는 modeling notation은 BPMN이므로, BPMN 2.0에 초점을 맞춤 Languages 전 세계인이 사용할 수 있으므로 각각 다른 언어를 사용하여 생성됨 → 절반 이상이 영어로 구성되어있음 Elements → BPMN 모델에서 발생하는 다양한 element types sequence flow 98.88% task 98.11 start message event 25.42% collapsed subprocess 25.23% .. 30개 이상의 element type은 전체의 1% 미만으로 사용됨 하나의 BPMN 모델에는 평균적으로 11.3개의 element type과 46.7개의 element가 사용됨 type별 element 수 Labels BPMN 모델의 모든 element는 모델러에 의해 label이 지정됨(28,293,762개 element에 대해 2,820,531개의 개별 label 생성) 첫 번째 bin에는 BPMN 모델의 element에 대해 가장 자주 사용되는 1만개의 label이 포함됨 전반적으로 전체 element 중 53.9%가 처음 1만개의 label로 지정되는 것을 알 수 있음(??) long-tail distribution은 많은 label이 모든 BPMN 모델에 한 element에만 사용됨을 의미(64.9%의 label이 한 번만 사용됨) unevenness of the label usage distribution(라벨 사용 분포의 불균일성) [ 5. SAP Signavio Academic Model Application ] 대규모 프로세스 모델 collection(i.e., SAP-SAM)은 BPM 연구를 위한 가치있고 중요한 자원임 이는 비즈니스 프로세스에 대한 조직적 지식과 모델링 실습에 대한 지식(knowledge)을 체계화함 BPM 커뮤니티에 대한 SAP-SAM의 potential value를 설명하기 위해 특히 관련있는 application scenario 설명 Knowledge Generation 프로세스 모델은 비즈니스 프로세스를 묘사하고 조직 운영에 대한 지식을 체계화함 SAP-SAM은 조직 모델링의 contents &amp;amp; practice에 대한 새로운 insight를 생성하는 지식 기반으로 간주될 수 있음 Application example Reference model mining [20] Identifying modeling patterns [10] Modeling Assistance Process model auto-completion [23] Automated abstraction techniques [27] Evaluation 평가는 조직 내 데이터셋과 비교할 수 있는 데이터셋으로 진행해야 함 SAP-SAM은 관련 데이터셋보다 훨씬 큰 데이터이므로 실제 데이터에 대한 기존 process management approach 평가에 사용 가능함 활용 예시 Process model querying[19] Process model matching [1] Process model similarity [6] [ 6. Limitations and Recommendations for Usage ] SAP-SAM은 학술 커뮤니티에서 광범위한 process querying/business process analytics use cases를 다루는 tool&amp;amp;algorithm을 테스트하고 평가하는 데 사용 가능하지만 데이터셋의 한계를 고려해야 함 SAP-SAM은 researcher, teacher, student에 의해 생성된 모델로 구성되어 있으므로 duplicate(copy)가 존재함 -&amp;gt; process querying에서 variant identification &amp;amp; fuzzy matching approach 평가에 사용할 수 있지만 diversity(i.e., the breadth of the dataset)에 부정적인 영향을 미침 많은 모델의 기술적 quality가 낮을 수 있음(학습 목적으로 학생 or process modeling beginner가 만든 모델이 포함되므로) -&amp;gt; 이러한 데이터에서 mistake 또는 antipattern을 분석할 수도 있지만 ML 기반의 generating modeling recommendation에는 적절하지 않을 수 있음 많은 모델이 교육 또는 학습, 시연 목적으로 만들어졌을 가능성이 높음 -&amp;gt; 교육, 학습, 시연 목적으로 생성된 모델은 technical precision &amp;amp; correctness를 강조하는 반면, industry process model은 facilitation of stakeholder alignment와 같은 business goal에 초점을 맞추므로 차이가 있음 이처럼 본 논문에서 제안하는 데이터셋이 exhaustive하지 않을 수 있음 특히, 이러한 한계는 특정 use case 또는 evaluation scenario에 따라 달라지므로 데이터셋 사용자가 이를 식별해야 함 대부분의 프로세스 모델은 A-B-C toy examples from exercises와 SAP-SAM 데이터셋을 넘어 연구를 용이하게 하는 방향으로 충분한 relevance &amp;amp; quality를 갖추고 있음[ 7. Conclusion ] SAP-SAM 데이터셋(현재까지 가장 큰 데이터 collection) 제시 process management professionals가 아닌 researcher, teacher, student로부터 생성된 “academic” 모델임에도 프로세스 모델 쿼리, 분석을 위한 tool &amp;amp; algorithm 개발, 평가에 유용할 것 Extension Business objects/dictionary entries Standard-conform XML serializations PNG/SVG image representations " }, { "title": "[논문리뷰] Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting", "url": "/posts/paper_ai_2/", "categories": "paper_review", "tags": "paper-review, ai, time-series-data, time-series, forecasting", "date": "2023-04-10 00:00:00 +0900", "snippet": " Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting(2019) google-research-TFT(github)# 1. Introduction" }, { "title": "[논문리뷰] A Review of Generalized Zero-Shot Learning Methods(2022)", "url": "/posts/paper_ai_1/", "categories": "paper_review", "tags": "paper-review, ai, image-data, zero-shot-learning", "date": "2023-04-03 00:00:00 +0900", "snippet": " Pourpanah, F., Abdar, M., Luo, Y., Zhou, X., Wang, R., Lim, C. P., … &amp;amp; Wu, Q. J. (2022). A review of generalized zero-shot learning methods. IEEE transactions on pattern analysis and machine intelligence. 논문 다운로드(arxiv)#[ 1. Introduction ] 기존 이미지 처리, 컴퓨터 비전 분야에서 딥러닝 모델의 발전 feature extraction에서 classification까지 end-to-end solution을 제공하는 기능으로 인기를 얻음 학습을 위해 여전히 많은 양의 sample과 class에 대한 label data를 필요로 함 이러한 대용량의 labeled data를 수집하는 것은 어렵고 training data에 있는 class가 아닌 경우(unseen data) 분류가 불가능하다는 문제가 존재함 모든 class에 대한 labeling 어려움(추가적인 도메인 지식 필요) 다양한 학습 구성을 위한 기술 One-shot learning Few-shot learning OSR(Open Set Recognition) : unseen class에 속하지만 정확한 class label 예측 불가 Out-of-distribution : training sample과 다른 test sample을 식별하고자 하지만 불가 인간은 대체로 약 3만개의 카테고리를 인식할 수 있으며, 모든 카테고리를 배우지 않고도 파악이 가능함 예) ‘말(horse)’을 본 적이 있는 아이는 얼룩말을 보고 ‘흑백 줄무늬가 있는 말’이라고 생각할 수 있음 → 다른 class data sample에서 얻은 knoledge를 사용하고 sample이 거의 없는 class 처리를 위한 classification model을 공식화 함 ZSL의 목표 seen class(source domain)로부터 얻은 transferring knowledge를 통해 unseen class(target domain)의 객체를 분류하는 학습 모델 구축 semantic information을 사용하여 seen/unseen class 간 격차 해소 seen, unseen class의 name을 고차원 벡터에 포함하며 아래의 벡터이거나 벡터들의 조합으로 구성 수동으로 정의된 attribute vector 자동으로 추출된 word vector context 기반의 embedding test data는 unseen class 로만 구성되지만, 실제로는 unseen class만 분류하는 것보다 seen/unseen class를 동시에 인식해야 함 ⇒ “GZSL; Generalized Zero Shot Learning” 1. Contributions ZSL 관련 연구 논문 [3], [24], [26], [27] 와의 차이점 [3]은 ZSL, few GZSL method에 초점을 맞춤 [24] ZSL, GZSL method에 대해 각각 서로 다른 case study(different data set 사용) 진행했으며, ZSL, GZSL method 자체보다 empirical research에 중점을 둠 [26] GZSL에 대한 간단한 논의 + ZSL에 초점을 맞춤 [27] COVID-19 질병을 위한 ZSL method의 중요성 강조 → ZSL보다 GZSL에 대한 in-depth survey and analysis를 포함한 기존 논문이 존재하지 않음 ⇒ 본 논문은 GZSL에 대한 포괄적인 review 제공(problem formulation, challenging issue, hierarchical categorization, applications..)을 목표로 함 The main contributions of this review paper include: comprehensive review of the GZSL methods, to the best of our knowledge, this is the first paper that attempts to provide an in-depth analysis of the GZSL methods; hierarchical categorization of the GZSL methods along with their corresponding representative models and real-world applications; elucidation on the main research gaps and suggestions for future research directions. 2. Organization 섹션별 주요 내용 Overview of GZSL problem formulation semantic information embedding spaces challenging issues Inductive and semantic transductive GZSL methods The transductive GZSL methods The applications of GZSL to various domains Discussion on the research gaps and trends for future research [ 2. Overview of Generalized Zero-Shot Learning ]1. Problem Formulation GZSL 학습 단계 Inductive learning seen class의 visual feature + semantic information 만으로 모델 구축 Transductive learning unseen class의 unlabeled visual feature + semantic information 활용 2. Performance Indicators Accuracy of seen (Acc s) Accuracy of unseen (Acc u) Area Under Seen-Unseen Accuracy Curve (AUSUC) AUSUC value 높을수록 GZSL task에서 balanced performance를 달성하는 것을 목표로 함 Harmonic Mean (HM) GZSL이 seen class 에 편향된 경우 ⇒ Acc s &amp;gt; Acc u ⇒ HM score 낮아짐 3. Semantic Information unseen class는 label이 없으므로 semantic information을 사용하여 seen class와 unseen class 간 관계를 구축 → ZSL에 활용 semantic information 은 모든 unseen class의 recognition properties를 포함해야하며, 사용성 보장을 위해 feature space의 sample과 관련이 있어야 함 종류 Manually defined attributes Word vectors ?.? 4. Embedding spaces Semantic Embedding class에 속하는 모든 이미지의 semantic embedding을 일부 ground-truth label embedding에 맵핑하도록 하는 것 nearest neighbor search를 사용하여 test image 인식 가능 Visual Embedding semantic representation을 해당 visual feature와 가깝게 만드는 것 nearest neighbor search를 사용하여 test image 인식 가능 Latent Embedding semantic/visual embedding space 사이의 explicit projection function을 학습하는 것은 어려움 → semantic/visual representation을 common space에 투영 intra-class compactness, inter-class separability를 만족해야 함 5. Challenging Issues Fig 4. (a) Ideal mapping result (b) Practical mapping result Fig 5. GZSL은 seen/unseen class 모두에 대한 인식을 수행하는 모델을 학습하므로, 일반적으로 seen class에 편향되고 unseen class data가 seen class로 잘못 분류됨 대부분의 ZSL은 이 문제를 효과적으로 해결할 수 없으므로, calibrated stacking, novelty detector..제안 calibrated stacking : seen/unseen class 모두에서 균형있게 인식하도록 함 scaled calibration &amp;amp; probabilistic representation 도 유사한 방법 detector : test sample이 seen/unseen 중 어느 class에 속하는지 식별하도록 함 [ 3. Review of GZSL Methods ]1. Embedding-based methods embedding space 학습 → seen class의 low-level visual feature을 해당 semantic vector와 연결 GZSL 문제를 해결하기 위한 다양한 embedding based method Out-of-distribution Detection-based Methods Graph-based Methods Meta Learning-based Methods Attention-based Methods Compositional Learning-based Methods Bidirectional Learning Methods Autoencoder-based Methods Other methods 2. Generative-based methods seen/unseen class의 semantic representation을 기반으로 unseen class에 대한 image/visual feature를 생성하는 모델 학습 Generative Adversarial networks Variational Autoencoders(VAEs) Combined GANs and VAEs Other Methods [ 4. Transductive GAZSL Methods ] unlabeled data에 접근 → 모델이 unseen class의 분포 파악 가능 ⇒ discriminative projection function 학습 가능1. Embedding-based methods Quasi-fully supervised learning(QFSL) 2. Generative-based methods[ 5. Applications ] 최근 CV, NLP에 적용 → image classification, object detection, video processing, NLP1. Computer vision image processing video processing2. Natural language processing[ 6. Discussions and Conclusions ] Highlights of GZSL methods Embedding 장점 구조가 덜 복잡하고 구현하기 쉬움 데이터셋 속성에 따라 linear, non-linear 등 다양한 projection function 선택 가능 단점 semantic embedding space 학습 시, hubness, shift, bias 문제 visual/latent space 학습 시, shift, bias 문제 Generative 장점 unseen class에 대한 많은 샘플 생성 가능 다양한 supervised model 사용 가능 단점 구조가 복잡하고 학습이 어려우며, 학습이 불안정해 mode collapse issue 존재 Highlights of embedding and generative-based methods A summary of ZSL methods" }, { "title": "[KoBART-summarization] fine-tuning 및 모델 추출 방법", "url": "/posts/kobartsum/", "categories": "envops", "tags": "", "date": "2022-04-14 00:00:00 +0900", "snippet": "뉴스 기사 요약 프로그램 개발 프로젝트를 진행하며 사용한 KoBART-summarization 사용 방법을 정리합니다.KoBART-summarizationKoBART 문서 에 따르면, KoBART는 BART 논문에서 사용된 Text Infilling 노이즈 함수를 사용하여 40GB 이상의 한국어 텍스트에 대해서 학습한 한국어 encoder-decoder 언어 모델이라고 합니다.이 포스트에서는, 현재 진행하고 있는 프로젝트에서 요약 모델을 fine-tuning해 사용하는 과정에서 발생한 문제에 대해 간단히 정리하려 합니다.[ fine-tuning 방법 ] 로컬 어딘가에 kobart clone할 폴더 위치에서 cmd 실행하고 kobart 프로젝트를 클론합니다. git clone https://github.com/seujung/KoBART-summarization.git 추가로 학습하고자 하는 데이터를 train/test로 나눈 후 tsv 형태로 저장하여 data 폴더 안에 넣습니다. csv -&amp;gt; tsv를 변환하는 편한(?) 방법은.. excel로 csv 파일을 열어서 [다른 이름으로 저장]할 때 텍스트(탭으로 분리)(*.txt) 로 저장한 후, 확장자명을 tsv로 수정해 저장하는 것인 것 같습니다. KoBART-summarization git README를 바탕으로 fine-tuning을 진행합니다. pip install -r requirements.txt GPU 사용 시 python train.py --gradient_clip_val 1.0 --max_epochs 50 --default_root_dir logs --gpus 1 --batch_size 4 --num_workers 4 또는 python train.py --gradient_clip_val 1.0 --max_epochs 50 --default_root_dir logs --strategy ddp --gpus 2 --batch_size 4 --num_workers 4 CPU 사용 시 python train.py --gradient_clip_val 1.0 --max_epochs 50 --default_root_dir logs --strategy ddp --batch_size 4 --num_workers 4 train.py 실행 중 발생하는 문제가 몇가지 있었는데, GPU 관련 문제는 CUDA 드라이버 및 CUDNN, tensorflow 등을 버전에 맞게 설치하니 해결되었습니다. KoBART-sum github에서 권장하는 CUDA 버전은 10.2입니다. (KoBART-summarization issue20) RuntimeError: ~ ‘indices’ 문제는 KoBART-summarization issue7 을 참고하세요. 아무런 문제 없이 학습이 시작되었다면, 다음으로 확인해야 할 사항은 데이터 형식이 원본 데이터(기존 tsv)와 일치하는지 입니다.제 경우에는 na.drop을 했음에도 비어있는 값이 있어서 오류가 발생했습니다.tsv 변환 전, 학습하고자 하는 데이터에 빈 열이 존재하지는 않는지 미리 확인하시면 학습이 중단되는 것을 방지할 수 있습니다. [ 모델 추출 방법 ] 학습이 끝나면 logs 폴더 내에 모델 파라미터에 대한 정보가 저장된 로그 파일이 생성됩니다. 생성된 로그 파일을 바탕으로 모델을 추출하기 위한 코드는 아래와 같습니다. KoBART-sum 위치에서 아래 코드를 실행하면 됩니다. python get_model_binary.py --hparams hparam_path --model_binary model_binary_path 이 때, hparam : ./logs/tb_logs/default/version_0/hparams.yaml 활용 model_binary: ./logs/kobart_summary-model_chp/epoch=02-val_loss=1.726.ckpt 활용 하므로 아래와 같이 각 path에 폴더의 경로를 넣어줍니다. python get_model_binary.py --hparams ./logs/tb_logs/default/version_0/hparams.yaml --model_binary ./logs/kobart_summary-model_chp/epoch=02-val_loss=1.726.ckpt 오류가 발생한다면 .logs 앞에 파일의 절대 경로를 넣어보세요 :) 위 코드가 정상적으로 동작했다면 아래와 같이 kobart_summary 폴더 내에 config.json, pytorch_model.bin 이 생성됩니다. 성공적으로 bin 파일을 얻었다면 다음을 실행해 새롭게 얻은 모델을 실행할 수 있습니다. streamlit run infer.py 코드 실행 시 http://localhost:8501/ 에 데모 페이지가 생성됩니다. infer.py 실행 시 발생 오류 1) KoBART 미설치 pip install git+https://github.com/SKT-AI/KoBART#egg=kobart 2) KoBART 설치 중 egg_info 오류 ``` # setuptools 업그레이드 pip install –upgrage setuptools # ez_setup 설치 pip install ez_setup # 설치 재시도 pip install unroll easy_install -U setuptools pip install unroll ```&amp;lt;!-- The Disqus lazy loading. --&amp;gt; Comments powered by Disqus. " }, { "title": "KoNLPy 분석기별 사용자 단어 추가 방법(windows)", "url": "/posts/konlpy/", "categories": "envops", "tags": "", "date": "2021-10-11 00:00:00 +0900", "snippet": "KoNLPy의 분석기별 사용자 단어를 추가하는 방법을 하나로 정리한 포스팅입니다.[ Kkma ] konlpy가 설치되어 있는 폴더로 이동합니다. 일반적으로C:/Users/%USERNAME/AppData/Roaming/Python/Python(user_version)/site-packages/konlpy/java kkma-2.0.jar 파일 압축을 해제합니다. 압축 해제 후 생성된 kkma-2.0/dic 폴더로 이동 후, 추가하려는 단어의 품사 파일(.dic)을 메모장으로 열어 수정합니다. 저장 후에는 반드시 dic의 상위 폴더인 kkma-2.0 폴더로 이동 후, cmd창에서 jar cvf 압축파일명.jar ./를 실행해 압축합니다. jar 옵션 - c : 새로운 jar 패키지 파일 생성 - v : 압축/해제 수행 관련 메세지 출력(생략 가능) - f : jar 파일명 지정 압축이 끝나면 kkma-2.0 폴더에 생성된 jar 파일을 kkma-2.0의 상위 폴더인 ~konlpy/java 폴더에 붙여넣고, 기존에 있던 kkma-2.0.jar 파일과 폴더를 삭제합니다. 조금 전 붙여넣은 jar 파일명을 kkma-2.0.jar로 변경한 후 konlpy 패키지를 불러오면 추가한 단어가 반영된 것을 확인할 수 있습니다. 간혹 새로 추가한 단어가 바로 반영되지 않는 경우가 있는 것 같습니다. 컴퓨터를 재부팅한 후 실행해보세요 :)[ Komoran ]komoran의 단어 추가 방식은 비교적 간단합니다. 아무 폴더에 txt 파일을 생성하여 사용자 사전을 만든 후 저장합니다.이 때 단어와 품사 사이는 \\t으로 분리합니다. 이후 komoran을 불러올 때 괄호에 아래와 같이 사전 경로를 지정하면 추가한 단어가 바로 적용되는 것을 확인할 수 있습니다.Komoran(userdic=&quot;사전경로.txt&quot;) [ Hannanum ] 한나눔의 사전 경로는 kkma, okt와 살짝 다릅니다.C:/Users/%USERNAME/AppData/Roaming/Python/Python(user_version)/site-packages/konlpy/java에서 data -&amp;gt; kE 폴더로 이동합니다. dic_user.txt를 메모장으로 열어 추가하고 싶은 단어를 추가합니다.komoran과 마찬가지로, 단어와 품사 사이는 \\t으로 분리합니다. 파일 저장 후 별도의 작업없이 적용하여 사용할 수 있습니다. 참고자료) 한나눔 한국어 형태소 분석기 사용자 매뉴얼[ Okt ]okt의 단어 추가 방식은 kkma와 동일합니다. konlpy 설치 경로로 이동합니다.C:/Users/%USERNAME/AppData/Roaming/Python/Python(user_version)/site-packages/konlpy/java open-korean-text-2.1.0.jar 파일의 압축을 해제합니다. 압축 해제 후 생성된 폴더에서 아래 경로로 이동합니다. 원하는 파일을 열어 단어를 추가합니다. 품사별로 폴더, 파일이 분리되어 있으므로 품사를 지정할 필요는 없습니다. kkma와 마찬가지로, 상위 폴더인 open-korean-text-2.1.0 폴더로 이동 후, cmd창에서 jar cvf 압축파일명.jar ./를 실행해 압축합니다. 압축이 끝나면 open-korean-text-2.1.0 폴더에 생성된 jar 파일을 ~konlpy/java 폴더에 붙여넣고, 기존에 있던 open-korean-text-2.1.0 jar 파일과 폴더를 삭제합니다. 조금 전 붙여넣은 jar 파일명을 open-korean-text-2.1.0.jar로 변경한 후 konlpy 패키지를 불러오면 추가한 단어가 반영된 것을 확인할 수 있습니다. 참고자료) Okt 빌드를 통한 사전 추가 Comments powered by Disqus. " } ]

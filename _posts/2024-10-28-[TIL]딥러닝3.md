---
layout: post
title:  "[TIL] 딥러닝 3. RNN"
subtitle:   ""
categories: [TIL] 
tag: [TIL, AI, RNN]
comments: true
---

LG Aimers 5기 AI 실습 교육 수강 내용을 정리합니다. 모든 자료는 교육 플랫폼 [앨리스](https://elice.io/ko)로부터 제공받았습니다.

## RNN(Recurrent Neural Network)
**RNN은 시계열 데이터와 같은 순차 데이터 처리를 위한 대표적인 딥러닝 모델**


### 1. 순차 데이터란?

**순서(Order)** 를 가지고 나타나는 데이터
데이터 내 **각 개체 간 순서**가 중요함

![순차 데이터 예시](https://github.com/user-attachments/assets/ceb3cc48-02ee-401f-9f85-044ece201fa4)

- 시계열 데이터(Time-Series)
    - **일정한 시간 간격**을 가지고 얻어낸 데이터
    - 예. 연도별 평균 기온, 시간별 주식 가격 등

- 자연어 데이터(Natural Language)
    - 인류가 말하는 언어
    - 주로 문장 내에서 **단어가 등장하는 순서**에 주목함

- 활용 분야
    - 경향성 파악 ex) 주가 예측, 기온 예측, 강수량 예측 등
    - 음악 장르 분석(오디오 파일 = 시계열)
    - 음성 인식(음성에 포함된 단어, 소리 추출)
    - 번역기
    - 챗봇

### 2. Recurrent Neural Network

**[ 등장 배경 ]**   

![](https://github.com/user-attachments/assets/e8559a01-b1fc-4d88-b58a-1c36b9a01ffc)

CNN에서 사용하는 Fully-connected Layer는 입/출력 노드 개수가 정해져 있음   
&rarr; 순차 데이터는 하나의 데이터를 이루는 개체 수가 다를 수 있음(ex. 서로 다른 개수의 단어로 구성된 문장들)   
&rarr; FC Layer 사용 불가   
&Rightarrow; 순차 데이터 처리를 위한 모델 등장

**[ 핵심 구성 요소 ]**   
- **Hidden State** : 순환 구조를 구현하는 장치

**[ 입력 데이터 구조 ]**
![](https://github.com/user-attachments/assets/909d5c7e-31d1-4db8-b35e-d09df6665365)

- 각 x<sub>t</sub>의 의미
    - 시계열 데이터 : 일정 시간 간격으로 나눠진 데이터에서 특정 시점의 값   
    - 자연어 데이터 : 문장 내의 각 단어


- 시계열 데이터 벡터 변환
![](https://github.com/user-attachments/assets/72785d8b-7d49-4485-8bab-354648bc628e)
    - 입력 데이터의 각 x<sub>t</sub>는 벡터 형태
    - 각 데이터를 이루는 Feature 값들을 원소로 벡터 변환
    
- 자연어 데이터 벡터 변환
![](https://github.com/user-attachments/assets/6ed2da92-4721-43dd-a2d8-e5a49e68b63d)

    - 임베딩(Embedding) 기법 : 각 단어들을 숫자 벡터로 변환   
        - One-hot encoding
        - Word2Vec

### 3. Vanilla RNN
- **가장 간단한 형태의 RNN 모델**을 의미함(Simple RNN)
- 내부에 3개의 FC Layer로 구성   
![](https://github.com/user-attachments/assets/b8ce812f-87cb-42c2-89ef-9478140b1364)
    - 𝑊<sub>ℎℎ</sub>: hidden state(𝒉<sub>𝒕−𝟏</sub>)를 변환하는 Layer의 가중치 행렬
    - 𝑊<sub>𝑥ℎ</sub>: 한 시점의 입력값(𝒙<sub>𝒕</sub>) 을 변환하는 Layer의 가중치 행렬
    - 𝑊<sub>ℎ𝑦</sub>: 한 시점의 출력값 (𝒚<sub>𝒕</sub>)을 변환하는 Layer의 가중치 행렬

**[ 연산 과정 ]**
- 모델에 들어오는 각 시점의 데이터 x<sub>t</sub>마다 hidden state, y<sub>t</sub> 를 반복적으로 계산   
![](https://github.com/user-attachments/assets/432c0157-8668-4869-8683-45ccf8681f14)
(실제로 사용된 RNN 모델은 1개이지만 순환 구조를 나타내기 위해 그림상 옆으로 펼쳐놓은 것)



- 𝒉<sub>𝒕</sub> 계산   
![](https://github.com/user-attachments/assets/352149ca-199a-4925-967e-00ac4c943400)
    - 현재 입력값(𝒙<sub>𝒕</sub>)에 대한 새로운 hidden state 계산
    - h<sub>t</sub> = tanh(h<sub>t-1</sub>W<sub>hh</sub> + x<sub>t</sub>W<sub>xh</sub>)
        ![](https://github.com/user-attachments/assets/548411e8-4e05-4d23-9b8a-eefac6c12373)
    (tanh는 tangent hyperbolic 함수)

- y<sub>𝒕</sub> 계산   
![](https://github.com/user-attachments/assets/2624a3ff-cae5-4dcf-94fa-e78eee0ca2ea)
    - 현재 입력값(𝒙<sub>𝒕</sub>)에 대한 새로운 출력값 계산
    - 𝒚<sub>𝒕</sub> = 𝑊<sub>ℎ𝑦</sub>H<sub>t</sub>
    (이때 앞에서 계산한 hidden state를 이용)

**[ 의의 ]**
- Hidden state의 의미
    - 특정 시점 t까지 들어온 입력값들의 **상관관계**나 **경향성 정보**를 압축해서 저장
    &rarr; 모델이 내부적으로 계속 업데이트 하는 값으로, 일종의 **메모리**로 볼 수 있음
- Parameter sharing
    - Hidden state와 출력값 계산을 위한 FC Layer를 모든 시점의 입력값이 재사용함
    - 3개의 FC Layer가 모델 파라미터의 전부인 구조

**[ 종류 ]**
- many-to-one : 여러 시점의 입력값으로 1개의 출력값 도출
- many-to-many : 여러 시점의 입력값, 여러 시점의 출력값(입/출력 개수는 같을수도, 다를수도 있음)
- encoder-decoder : 입력값을 받아 특정 hidden state로 인코딩 후, hidden state로부터 새로운 출력값을 만드는 구조

![종류1](https://github.com/user-attachments/assets/b1e6d067-5b4e-47f4-8270-907ab5343196)
![종류2](https://github.com/user-attachments/assets/36d2973b-53d3-4c37-ba78-a7cd1fd62b0b)


**[ 문제점 ]**

![](https://github.com/user-attachments/assets/8e48dbf5-7119-498e-9c8e-17a9b4e23888)
- 시간 순서에 따라 생성되는 출력값
- 각 시점의 출력값과 실제값을 비교하여 손실(loss)값을 계산함
- 역전파 알고리즘이 시간에 따라 작동 &rarr; BPTT; Back-propagation Through Time

![](https://github.com/user-attachments/assets/4d0244f9-f93e-4b1c-86b8-5e6297e067e3)
- 입력값의 길이가 길어질수록 초기 입력값과 나중 출력값 사이에 전파되는 기울기 값이 매우 작아질 가능성이 높음   
&rarr; 기울기 소실 문제(Vanishing Gradient Problem) 발생 가능
(=장기 의존성 다루기가 어려움)
---
layout: post
title:  "[TIL] 딥러닝 3. RNN 실습(Python)"
subtitle:   ""
categories: [TIL] 
tag: [TIL, AI]
comments: true
---

LG Aimers 5기 AI 실습 교육 수강 내용을 정리합니다. 모든 자료는 교육 플랫폼 [앨리스](https://elice.io/ko)로부터 제공받았습니다.

## RNN(Recurrent Neural Network)
**RNN은 시계열 데이터와 같은 순차 데이터 처리를 위한 대표적인 딥러닝 모델**

지난 시간 학습한 RNN을 파이썬으로 구현해 보았습니다.

### [ Vanilla RNN 코드 ]

<div markdown="1">

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' ## 텐서플로우 간단한 경고 뜨지 않도록 설정

import tensorflow as tf
from tensorflow.keras import layers, Sequential

def build_model1():
    '''
    layers.Embedding
    - 전체 단어 개수: 10개
    - 벡터 길이: 5

    layers.SimpleRNN
    - hidden state의 크기: 3
    '''
    model = Sequential()
    
    model.add(layers.Embedding(10, 5)) ## input_dim, output_dim
    model.add(layers.SimpleRNN(3)) ## hidden_state(=vector) 크기
    
    return model

def build_model2():
    '''
    layers.Embedding
    - 전체 단어 개수: 256개
    - 벡터 길이: 100
    layers.SimpleRNN
    - hidden state의 크기: 20
    layers.Dense
    - 노드 개수: 10
    - 활성화 함수: softmax
    '''
    model = Sequential()
    
    model.add(layers.Embedding(256, 100))
    model.add(layers.SimpleRNN(20))
    model.add(layers.Dense(10, activation = 'softmax')) ## RNN으로 분류하기 위해 Dense layer 추가
    
    return model
    
def main():
    model1 = build_model1()
    print("=" * 20, "첫번째 모델", "=" * 20)
    model1.summary()
    ## 모델 성능에 도움이 되는 벡터로 임베딩되고, 그 벡터로 학습함

    print()
    
    model2 = build_model2()
    print("=" * 20, "두번째 모델", "=" * 20)
    model2.summary()

if __name__ == "__main__":
    main()

```
</div>


### [ Vanilla RNN으로 IMDb 데이터 학습하기 ]
- Dataset : IMDb 영화별 리뷰에 대한 긍/부정 데이터
- Task : 감성 분석(Sentimental Analysis)

<div markdown="1">

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
from tensorflow.keras import layers, Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

def load_data(num_words, max_len):
    # imdb 데이터셋을 불러옵니다.
    # 데이터셋에서 단어는 num_words 개를 가져옵니다.
    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)

    # 단어 개수가 다른 문장들을 Padding을 추가하여
    # 단어가 가장 많은 문장의 단어 개수로 통일합니다.
    X_train = pad_sequences(X_train, maxlen=max_len)
    X_test = pad_sequences(X_test, maxlen=max_len)
    
    return X_train, X_test, y_train, y_test

def build_rnn_model(num_words, embedding_len):
    model = Sequential()
    
    model.add(layers.Embedding(num_words, embedding_len))
    model.add(layers.SimpleRNN(16))
    model.add(layers.Dense(1, activation="sigmoid")) # 분류
    # sigmoid를 통해 도출된 0~1 사이의 값(=긍정일 확률)
    
    return model

def main(model=None, epochs=5):
    # IMDb 데이터셋에서 가져올 단어의 개수
    num_words = 6000
    
    # 각 문장이 가질 수 있는 최대 단어 개수
    max_len = 130
    
    # 임베딩 된 벡터의 길이
    embedding_len = 100
    
    # IMDb 데이터셋을 불러옵니다.
    X_train, X_test, y_train, y_test = load_data(num_words, max_len)
    
    if model is None:
        model = build_rnn_model(num_words, embedding_len)
    
    optimizer = Adam(learning_rate = 0.001)
    model.compile(optimizer = optimizer, loss = "binary_crossentropy", metrics = ["accuracy"])
    
    hist = model.fit(X_train, y_train, epochs=epochs, batch_size = 100, validation_split = 0.2, shuffle = True, verbose = 2)
    
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print()
    print("테스트 Loss: {:.5f}, 테스트 정확도: {:.3f}%".format(test_loss, test_acc * 100))
    
    return optimizer, hist

if __name__=="__main__":
    main()

</div>